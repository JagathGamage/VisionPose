{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQvmsyijFxMm"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DiDPLNDdFPvm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Mar 16 13:23:02 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 572.60                 Driver Version: 572.60         CUDA Version: 12.8     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce MX330         WDDM  |   00000000:02:00.0 Off |                  N/A |\n",
            "| N/A   62C    P0            N/A  / 5001W |       0MiB /   2048MiB |      2%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dqX5bzLbFs-0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch:  2.5 ; cuda:  cu121\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'nvcc' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Y-eoyQbZF_PM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Jagath\\Desktop\\pose\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-v532W2GD_t"
      },
      "source": [
        "# YOLOv7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bL1BAojZGJXA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Jagath\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Jagath\\Desktop\\pose\n",
            "c:\\Users\\Jagath\\Desktop\\pose\\yolov7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'yolov7' already exists and is not an empty directory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib>=3.2.2 in c:\\python312\\lib\\site-packages (from -r requirements.txt (line 4)) (3.10.1)\n",
            "Collecting numpy<1.24.0,>=1.18.5 (from -r requirements.txt (line 5))\n",
            "  Using cached numpy-1.23.5.tar.gz (10.7 MB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'error'\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Getting requirements to build wheel did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [33 lines of output]\n",
            "      Traceback (most recent call last):\n",
            "        File \"c:\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
            "          main()\n",
            "        File \"c:\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
            "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
            "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "        File \"c:\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 112, in get_requires_for_build_wheel\n",
            "          backend = _build_backend()\n",
            "                    ^^^^^^^^^^^^^^^^\n",
            "        File \"c:\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 77, in _build_backend\n",
            "          obj = import_module(mod_path)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "        File \"c:\\Python312\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
            "          return _bootstrap._gcd_import(name[level:], package, level)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "        File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "        File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "        File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
            "        File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "        File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "        File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "        File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
            "        File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
            "        File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
            "        File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "        File \"C:\\Users\\Jagath\\AppData\\Local\\Temp\\pip-build-env-gjm1ryw_\\overlay\\Lib\\site-packages\\setuptools\\__init__.py\", line 16, in <module>\n",
            "          import setuptools.version\n",
            "        File \"C:\\Users\\Jagath\\AppData\\Local\\Temp\\pip-build-env-gjm1ryw_\\overlay\\Lib\\site-packages\\setuptools\\version.py\", line 1, in <module>\n",
            "          import pkg_resources\n",
            "        File \"C:\\Users\\Jagath\\AppData\\Local\\Temp\\pip-build-env-gjm1ryw_\\overlay\\Lib\\site-packages\\pkg_resources\\__init__.py\", line 2172, in <module>\n",
            "          register_finder(pkgutil.ImpImporter, find_on_path)\n",
            "                          ^^^^^^^^^^^^^^^^^^^\n",
            "      AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "error: subprocess-exited-with-error\n",
            "\n",
            "× Getting requirements to build wheel did not run successfully.\n",
            "│ exit code: 1\n",
            "╰─> See above for output.\n",
            "\n",
            "note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%cd {HOME}\n",
        "!git clone https://github.com/WongKinYiu/yolov7\n",
        "%cd {HOME}/yolov7\n",
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6D3zGdYOGLyJ"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(f\"{HOME}/yolov7\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA2aIiWmGPBY"
      },
      "source": [
        "# Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zLqn9syzGTUI"
      },
      "outputs": [],
      "source": [
        "# %cd {HOME}\n",
        "# !mkdir input\n",
        "# %cd {HOME}/input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tJUw77irGYun"
      },
      "outputs": [],
      "source": [
        "SOURCE_VIDEO_A_PATH = f\"{HOME}/input/vid1.mp4\"\n",
        "SOURCE_VIDEO_B_PATH = f\"{HOME}/input/vid-mid.mp4\"\n",
        "SOURCE_VIDEO_C_PATH = f\"{HOME}/input/vid-right.mp4\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W5KVlYmGeXh"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MMa-1mxeGjLX"
      },
      "outputs": [],
      "source": [
        "from typing import Generator\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import cv2\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "def generate_frames(video_file: str) -> Generator[np.ndarray, None, None]:\n",
        "    video = cv2.VideoCapture(video_file)\n",
        "\n",
        "    while video.isOpened():\n",
        "        success, frame = video.read()\n",
        "\n",
        "        if not success:\n",
        "            break\n",
        "\n",
        "        yield frame\n",
        "\n",
        "    video.release()\n",
        "\n",
        "\n",
        "def plot_image(image: np.ndarray, size: int = 12) -> None:\n",
        "    plt.figure(figsize=(size, size))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image[...,::-1])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-D6170K0Gmiw"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Tuple\n",
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Point:\n",
        "    x: float\n",
        "    y: float\n",
        "\n",
        "    @property\n",
        "    def int_xy_tuple(self) -> Tuple[int, int]:\n",
        "        return int(self.x), int(self.y)\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Rect:\n",
        "    x: float\n",
        "    y: float\n",
        "    width: float\n",
        "    height: float\n",
        "\n",
        "    @property\n",
        "    def top_left(self) -> Point:\n",
        "        return Point(x=self.x, y=self.y)\n",
        "\n",
        "    @property\n",
        "    def bottom_right(self) -> Point:\n",
        "        return Point(x=self.x + self.width, y=self.y + self.height)\n",
        "\n",
        "    @property\n",
        "    def bottom_center(self) -> Point:\n",
        "        return Point(x=self.x + self.width / 2, y=self.y + self.height)\n",
        "\n",
        "@dataclass\n",
        "class Detection:\n",
        "    rect: Rect\n",
        "    class_id: int\n",
        "    confidence: float\n",
        "    tracker_id: Optional[int] = None\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Color:\n",
        "    r: int\n",
        "    g: int\n",
        "    b: int\n",
        "\n",
        "    @property\n",
        "    def bgr_tuple(self) -> Tuple[int, int, int]:\n",
        "        return self.b, self.g, self.r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cs_m3lPRGqHa"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def draw_rect(image: np.ndarray, rect: Rect, color: Color, thickness: int = 2) -> np.ndarray:\n",
        "    cv2.rectangle(image, rect.top_left.int_xy_tuple, rect.bottom_right.int_xy_tuple, color.bgr_tuple, thickness)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oBf2E0hIGtjg"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1600x1600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "frame_iterator = iter(generate_frames(video_file=SOURCE_VIDEO_A_PATH))\n",
        "\n",
        "frame = next(frame_iterator)\n",
        "plot_image(frame, 16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3llAwMDoGwU0"
      },
      "source": [
        "# Load YOLOv7 Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOeap7XnG1-r"
      },
      "source": [
        "Download Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eD6ALDreG4nR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Jagath\\Desktop\\pose\\yolov7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Jagath\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "%cd {HOME}/yolov7\n",
        "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6e.pt --quiet\n",
        "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6-pose.pt --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Wq6TPfmBG6fr"
      },
      "outputs": [],
      "source": [
        "DETECTION_MODEL_WEIGHTS_PATH = f\"{HOME}/yolov7/yolov7-e6e.pt\"\n",
        "POSE_MODEL_WEIGHTS_PATH = f\"{HOME}/yolov7/yolov7-w6-pose.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "P6mBQ8uBG8u8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw_mz5K7HAyd"
      },
      "source": [
        "Load Detection Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9esFjRVbHCrc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Jagath\\Desktop\\pose\\yolov7\\models\\experimental.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(w, map_location=map_location)  # load\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fusing layers... \n"
          ]
        }
      ],
      "source": [
        "from utils.general import check_img_size\n",
        "from models.experimental import attempt_load\n",
        "\n",
        "detection_model = attempt_load(weights=DETECTION_MODEL_WEIGHTS_PATH, map_location=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBfU-Rx2HEsj"
      },
      "source": [
        "Load Pose Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "L_MxhlrxHGWd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Jagath\\AppData\\Local\\Temp\\ipykernel_15808\\2097434443.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  weigths = torch.load(POSE_MODEL_WEIGHTS_PATH, map_location=device)\n"
          ]
        }
      ],
      "source": [
        "weigths = torch.load(POSE_MODEL_WEIGHTS_PATH, map_location=device)\n",
        "pose_model = weigths[\"model\"]\n",
        "_ = pose_model.float().eval()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    pose_model.half().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lhEOdUiHKLX"
      },
      "source": [
        "# Single Frame Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WAYboJ8HOxE"
      },
      "source": [
        "Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LmV-_P6vHN3b"
      },
      "outputs": [],
      "source": [
        "DETECTION_IMAGE_SIZE = 1920\n",
        "POSE_IMAGE_SIZE = 960\n",
        "STRIDE = 64\n",
        "CONFIDENCE_TRESHOLD = 0.25\n",
        "IOU_TRESHOLD = 0.65"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjeFOLhXHTCu"
      },
      "source": [
        "Pre-process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "18SGeybSHURA"
      },
      "outputs": [],
      "source": [
        "from utils.datasets import letterbox\n",
        "from torchvision import transforms\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def detection_pre_process_frame(frame: np.ndarray, device: torch.device) -> torch.Tensor:\n",
        "    img = letterbox(frame, DETECTION_IMAGE_SIZE, STRIDE, auto=True)[0]\n",
        "    img = img[:, :, ::-1].transpose(2, 0, 1)\n",
        "    img = np.ascontiguousarray(img)\n",
        "    img = torch.from_numpy(img).to(device).float()\n",
        "    img /= 255.0\n",
        "    if img.ndimension() == 3:\n",
        "        img = img.unsqueeze(0)\n",
        "    return img\n",
        "\n",
        "def pose_pre_process_frame(frame: np.ndarray, device: torch.device) -> torch.Tensor:\n",
        "    image = letterbox(frame, POSE_IMAGE_SIZE, stride=STRIDE, auto=True)[0]\n",
        "    image = transforms.ToTensor()(image)\n",
        "    image = torch.tensor(np.array([image.numpy()]))\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        image = image.half().to(device)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9CK4GajHZi3"
      },
      "source": [
        "Post-process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pYweiAE3HYmu"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "from utils.general import non_max_suppression_kpt, non_max_suppression\n",
        "from utils.plots import output_to_keypoint\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def clip_coords(boxes: np.ndarray, img_shape: Tuple[int, int]):\n",
        "    # Clip bounding xyxy bounding boxes to image shape (height, width)\n",
        "    boxes[:, 0] = np.clip(boxes[:, 0], 0, img_shape[1]) # x1\n",
        "    boxes[:, 1] = np.clip(boxes[:, 1], 0, img_shape[0]) # y1\n",
        "    boxes[:, 2] = np.clip(boxes[:, 2], 0, img_shape[1]) # x2\n",
        "    boxes[:, 3] = np.clip(boxes[:, 3], 0, img_shape[0]) # y2\n",
        "\n",
        "\n",
        "def detection_post_process_output(\n",
        "    output: torch.tensor,\n",
        "    confidence_trashold: float,\n",
        "    iou_trashold: float,\n",
        "    image_size: Tuple[int, int],\n",
        "    scaled_image_size: Tuple[int, int]\n",
        ") -> np.ndarray:\n",
        "    output = non_max_suppression(\n",
        "        prediction=output,\n",
        "        conf_thres=confidence_trashold,\n",
        "        iou_thres=iou_trashold\n",
        "    )\n",
        "    coords = output[0].detach().cpu().numpy()\n",
        "\n",
        "    v_gain = scaled_image_size[0] / image_size[0]\n",
        "    h_gain = scaled_image_size[1] / image_size[1]\n",
        "\n",
        "    coords[:, 0] /= h_gain\n",
        "    coords[:, 1] /= v_gain\n",
        "    coords[:, 2] /= h_gain\n",
        "    coords[:, 3] /= v_gain\n",
        "\n",
        "    clip_coords(coords, image_size)\n",
        "    return coords\n",
        "\n",
        "\n",
        "def post_process_pose(pose: np.ndarray, image_size: Tuple, scaled_image_size: Tuple) -> np.ndarray:\n",
        "    height, width = image_size\n",
        "    scaled_height, scaled_width = scaled_image_size\n",
        "    vertical_factor = height / scaled_height\n",
        "    horizontal_factor = width / scaled_width\n",
        "    result = pose.copy()\n",
        "    for i in range(17):\n",
        "        result[i * 3] = horizontal_factor * result[i * 3]\n",
        "        result[i * 3 + 1] = vertical_factor * result[i * 3 + 1]\n",
        "    return result\n",
        "\n",
        "\n",
        "def pose_post_process_output(\n",
        "    output: torch.tensor,\n",
        "    confidence_trashold: float,\n",
        "    iou_trashold: float,\n",
        "    image_size: Tuple[int, int],\n",
        "    scaled_image_size: Tuple[int, int]\n",
        ") -> np.ndarray:\n",
        "    output = non_max_suppression_kpt(\n",
        "        prediction=output,\n",
        "        conf_thres=confidence_trashold,\n",
        "        iou_thres=iou_trashold,\n",
        "        nc=pose_model.yaml['nc'],\n",
        "        nkpt=pose_model.yaml['nkpt'],\n",
        "        kpt_label=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = output_to_keypoint(output)\n",
        "\n",
        "        for idx in range(output.shape[0]):\n",
        "            output[idx, 7:] = post_process_pose(\n",
        "                output[idx, 7:],\n",
        "                image_size=image_size,\n",
        "                scaled_image_size=scaled_image_size\n",
        "            )\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXVzZBQJHiVh"
      },
      "source": [
        "Annotate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "exjK86Q3HjUh"
      },
      "outputs": [],
      "source": [
        "from utils.plots import plot_skeleton_kpts\n",
        "\n",
        "\n",
        "def detect_annotate(image: np.ndarray, detections: np.ndarray, color: Color, thickness: int = 2) -> np.ndarray:\n",
        "    annotated_image = image.copy()\n",
        "    for x_min, y_min, x_max, y_max, confidence, class_id in detections:\n",
        "        rect = Rect(\n",
        "            x=float(x_min),\n",
        "            y=float(y_min),\n",
        "            width=float(x_max - x_min),\n",
        "            height=float(y_max - y_min)\n",
        "        )\n",
        "        annotated_image = draw_rect(image=annotated_image, rect=rect, color=color, thickness=thickness)\n",
        "\n",
        "    return annotated_image\n",
        "\n",
        "\n",
        "def pose_annotate(image: np.ndarray, detections: np.ndarray) -> np.ndarray:\n",
        "    annotated_frame = image.copy()\n",
        "\n",
        "    # for idx in range(detections.shape[0]):\n",
        "    #     pose = detections[idx, 7:].T\n",
        "    #     plot_skeleton_kpts(annotated_frame, pose, 3)\n",
        "\n",
        "    # Iterate over each detected person in the detections\n",
        "    for idx in range(detections.shape[0]):\n",
        "        # Extract keypoints for the pose\n",
        "        pose = detections[idx, 7:]  # Extract keypoints\n",
        "        if pose.ndim > 1:           # Flatten to 1D if needed\n",
        "            pose = pose.flatten()\n",
        "\n",
        "        plot_skeleton_kpts(annotated_frame, pose, 3)  # Plot skeleton keypoints\n",
        "\n",
        "        # Draw each keypoint's index near the keypoint on the frame\n",
        "        for i in range(len(pose) // 3):  # Iterate over each keypoint\n",
        "            x = int(pose[i * 3])         # x-coordinate of the keypoint\n",
        "            y = int(pose[i * 3 + 1])     # y-coordinate of the keypoint\n",
        "            conf = pose[i * 3 + 2]       # confidence score\n",
        "\n",
        "            if conf > 0.5:  # Only display if the confidence is above a threshold\n",
        "                cv2.putText(\n",
        "                    annotated_frame,\n",
        "                    str(i),                # Text to display (index of keypoint)\n",
        "                    (x, y),                # Position to display the text\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    0.5,                   # Font scale\n",
        "                    (255, 255, 255),           # Color (White)\n",
        "                    1,                     # Thickness\n",
        "                    cv2.LINE_AA\n",
        "                )\n",
        "\n",
        "    return annotated_frame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hg-DuvWqHm5Y"
      },
      "source": [
        "Single Frame Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "RS4ScyA2HpAH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python312\\Lib\\site-packages\\torch\\functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3596.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "C:\\Users\\Jagath\\AppData\\Local\\Temp\\ipykernel_15808\\2037076568.py:29: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "COLOR = Color(r=255, g=255, b=255)\n",
        "\n",
        "frame_iterator = iter(generate_frames(video_file=SOURCE_VIDEO_A_PATH))\n",
        "\n",
        "frame = next(frame_iterator)\n",
        "\n",
        "detection_pre_processed_frame = detection_pre_process_frame(\n",
        "    frame=frame,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "image_size = frame.shape[:2]\n",
        "scaled_image_size = tuple(detection_pre_processed_frame.size())[2:]\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    detection_output = detection_model(detection_pre_processed_frame)[0].detach().cpu()\n",
        "\n",
        "    detection_output = detection_post_process_output(\n",
        "        output=detection_output,\n",
        "        confidence_trashold=CONFIDENCE_TRESHOLD,\n",
        "        iou_trashold=IOU_TRESHOLD,\n",
        "        image_size=image_size,\n",
        "        scaled_image_size=scaled_image_size\n",
        "    )\n",
        "\n",
        "annotated_frame = detect_annotate(image=frame, detections=detection_output, color=COLOR)\n",
        "\n",
        "plot_image(annotated_frame, 16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gywrvwxwHtDJ"
      },
      "source": [
        "Single Frame Pose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "7LIjmdGkHugT"
      },
      "outputs": [],
      "source": [
        "def process_frame_and_annotate(frame: np.ndarray) -> np.ndarray:\n",
        "    pose_pre_processed_frame = pose_pre_process_frame(frame=frame.copy(), device=device)\n",
        "\n",
        "    image_size = frame.shape[:2]\n",
        "    scaled_image_size = tuple(pose_pre_processed_frame.size())[2:]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pose_output, _ = pose_model(pose_pre_processed_frame)\n",
        "        pose_output = pose_post_process_output(\n",
        "            output=pose_output,\n",
        "            confidence_trashold=CONFIDENCE_TRESHOLD,\n",
        "            iou_trashold=IOU_TRESHOLD,\n",
        "            image_size=image_size,\n",
        "            scaled_image_size=scaled_image_size\n",
        "        )\n",
        "\n",
        "    annotated_frame = pose_annotate(image=frame, detections=pose_output)\n",
        "\n",
        "    return annotated_frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "T9r8Z9rrHwz7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Jagath\\AppData\\Local\\Temp\\ipykernel_15808\\2037076568.py:29: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "frame_iterator = iter(generate_frames(video_file=SOURCE_VIDEO_A_PATH))\n",
        "\n",
        "frame = next(frame_iterator)\n",
        "\n",
        "annotated_frame = process_frame_and_annotate(frame=frame)\n",
        "\n",
        "plot_image(annotated_frame, 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "SfKaYIwNHzNh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Jagath\\AppData\\Local\\Temp\\ipykernel_15808\\2037076568.py:29: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "frame_iterator = iter(generate_frames(video_file=SOURCE_VIDEO_A_PATH))\n",
        "\n",
        "frame = next(frame_iterator)\n",
        "\n",
        "annotated_frame = process_frame_and_annotate(frame=frame)\n",
        "\n",
        "plot_image(annotated_frame, 16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyweEnEhH2ZT"
      },
      "source": [
        "# Process Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "HF742P31H41M"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "import cv2\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "usage example:\n",
        "\n",
        "video_config = VideoConfig(\n",
        "    fps=30,\n",
        "    width=1920,\n",
        "    height=1080)\n",
        "video_writer = get_video_writer(\n",
        "    target_video_path=TARGET_VIDEO_PATH,\n",
        "    video_config=video_config)\n",
        "\n",
        "for frame in frames:\n",
        "    ...\n",
        "    video_writer.write(frame)\n",
        "\n",
        "video_writer.release()\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# stores information about output video file, width and height of the frame must be equal to input video\n",
        "@dataclass(frozen=True)\n",
        "class VideoConfig:\n",
        "    fps: float\n",
        "    width: int\n",
        "    height: int\n",
        "\n",
        "\n",
        "# create cv2.VideoWriter object that we can use to save output video\n",
        "def get_video_writer(target_video_path: str, video_config: VideoConfig) -> cv2.VideoWriter:\n",
        "    video_target_dir = os.path.dirname(os.path.abspath(target_video_path))\n",
        "    os.makedirs(video_target_dir, exist_ok=True)\n",
        "    return cv2.VideoWriter(\n",
        "        target_video_path,\n",
        "        fourcc=cv2.VideoWriter_fourcc(*\"H264\"),\n",
        "        fps=video_config.fps,\n",
        "        frameSize=(video_config.width, video_config.height),\n",
        "        isColor=True\n",
        "    )\n",
        "\n",
        "\n",
        "def get_frame_count(path: str) -> int:\n",
        "    cap = cv2.VideoCapture(SOURCE_VIDEO_B_PATH)\n",
        "    return int(cap.get(cv2.CAP_PROP_FRAME_COUNT))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "YM31hvPqH7bI"
      },
      "outputs": [],
      "source": [
        "SOURCE_VIDEO_PATH = SOURCE_VIDEO_A_PATH\n",
        "TARGET_VIDEO_PATH = f\"{HOME}/output/pose-estimation-synchronised-sample-b.mp4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "JqNF5FWtH9lD"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0fb2068df261453d9d05320dca90e824",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[27], line 52\u001b[0m\n\u001b[0;32m     47\u001b[0m pose_pre_processed_frame \u001b[38;5;241m=\u001b[39m pose_pre_process_frame(\n\u001b[0;32m     48\u001b[0m     frame\u001b[38;5;241m=\u001b[39mframe,\n\u001b[0;32m     49\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     50\u001b[0m pose_scaled_image_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(pose_pre_processed_frame\u001b[38;5;241m.\u001b[39msize())[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m---> 52\u001b[0m pose_output \u001b[38;5;241m=\u001b[39m \u001b[43mpose_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpose_pre_processed_frame\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m     53\u001b[0m pose_output \u001b[38;5;241m=\u001b[39m pose_post_process_output(\n\u001b[0;32m     54\u001b[0m     output\u001b[38;5;241m=\u001b[39mpose_output,\n\u001b[0;32m     55\u001b[0m     confidence_trashold\u001b[38;5;241m=\u001b[39mCONFIDENCE_TRESHOLD,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m     scaled_image_size\u001b[38;5;241m=\u001b[39mpose_scaled_image_size\n\u001b[0;32m     59\u001b[0m )\n\u001b[0;32m     60\u001b[0m annotated_frame \u001b[38;5;241m=\u001b[39m pose_annotate(\n\u001b[0;32m     61\u001b[0m     image\u001b[38;5;241m=\u001b[39mannotated_frame, detections\u001b[38;5;241m=\u001b[39mpose_output)\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\Jagath\\Desktop\\pose\\yolov7\\models\\yolo.py:599\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x, augment, profile)\u001b[0m\n\u001b[0;32m    597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# augmented inference, train\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Jagath\\Desktop\\pose\\yolov7\\models\\yolo.py:625\u001b[0m, in \u001b[0;36mModel.forward_once\u001b[1;34m(self, x, profile)\u001b[0m\n\u001b[0;32m    622\u001b[0m         dt\u001b[38;5;241m.\u001b[39mappend((time_synchronized() \u001b[38;5;241m-\u001b[39m t) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m    623\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%10.1f\u001b[39;00m\u001b[38;5;132;01m%10.0f\u001b[39;00m\u001b[38;5;132;01m%10.1f\u001b[39;00m\u001b[38;5;124mms \u001b[39m\u001b[38;5;132;01m%-40s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (o, m\u001b[38;5;241m.\u001b[39mnp, dt[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], m\u001b[38;5;241m.\u001b[39mtype))\n\u001b[1;32m--> 625\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\Jagath\\Desktop\\pose\\yolov7\\models\\common.py:108\u001b[0m, in \u001b[0;36mConv.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "\n",
        "COLOR = Color(r=255, g=255, b=255)\n",
        "\n",
        "# initiate video writer\n",
        "video_config = VideoConfig(\n",
        "    fps=25,\n",
        "    width=1920,\n",
        "    height=1080)\n",
        "video_writer = get_video_writer(\n",
        "    target_video_path=TARGET_VIDEO_PATH,\n",
        "    video_config=video_config)\n",
        "\n",
        "# get fresh video frame generator\n",
        "frame_iterator = iter(generate_frames(video_file=SOURCE_VIDEO_PATH))\n",
        "\n",
        "total = get_frame_count(SOURCE_VIDEO_PATH)\n",
        "\n",
        "for i,frame in enumerate(tqdm(frame_iterator, total=total)):\n",
        "    start_time = time.time()\n",
        "    # print(f\"Processing fame {i+1}/{total}...\")\n",
        "\n",
        "    annotated_frame = frame.copy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_size = frame.shape[:2]\n",
        "\n",
        "        #detection\n",
        "        detection_pre_processed_frame = detection_pre_process_frame(\n",
        "            frame=frame,\n",
        "            device=device)\n",
        "        detection_scaled_image_size = tuple(detection_pre_processed_frame.size())[2:]\n",
        "\n",
        "        detection_output = detection_model(detection_pre_processed_frame)[0].detach().cpu()\n",
        "        detection_output = detection_post_process_output(\n",
        "            output=detection_output,\n",
        "            confidence_trashold=CONFIDENCE_TRESHOLD,\n",
        "            iou_trashold=IOU_TRESHOLD,\n",
        "            image_size=image_size,\n",
        "            scaled_image_size=detection_scaled_image_size\n",
        "        )\n",
        "        annotated_frame = detect_annotate(\n",
        "            image=annotated_frame, detections=detection_output, color=COLOR)\n",
        "\n",
        "        # pose\n",
        "        pose_pre_processed_frame = pose_pre_process_frame(\n",
        "            frame=frame,\n",
        "            device=device)\n",
        "        pose_scaled_image_size = tuple(pose_pre_processed_frame.size())[2:]\n",
        "\n",
        "        pose_output = pose_model(pose_pre_processed_frame)[0].detach().cpu()\n",
        "        pose_output = pose_post_process_output(\n",
        "            output=pose_output,\n",
        "            confidence_trashold=CONFIDENCE_TRESHOLD,\n",
        "            iou_trashold=IOU_TRESHOLD,\n",
        "            image_size=image_size,\n",
        "            scaled_image_size=pose_scaled_image_size\n",
        "        )\n",
        "        annotated_frame = pose_annotate(\n",
        "            image=annotated_frame, detections=pose_output)\n",
        "\n",
        "\n",
        "        # Log annotated frame dimensions\n",
        "        # print(f\"Annotated frame dimensions before resize: {annotated_frame.shape}\")\n",
        "\n",
        "        # Resize the annotated frame if dimensions do not match (again)\n",
        "        if annotated_frame.shape[:2] != (1080, 1920):\n",
        "            # print(f\"Resizing annotated frame from {annotated_frame.shape[:2]} to (1080, 1920)...\")\n",
        "            annotated_frame = cv2.resize(annotated_frame, (1920, 1080))\n",
        "\n",
        "        # Check if the annotated frame's dimensions are valid after resize\n",
        "        # print(f\"Annotated frame dimensions after resize: {annotated_frame.shape}\")\n",
        "\n",
        "        # Check if frame is valid (not None) and has the right dimensions\n",
        "        if annotated_frame is not None and annotated_frame.shape[:2] == (1080, 1920):\n",
        "            video_writer.write(annotated_frame)\n",
        "        else:\n",
        "            print(f\"Frame {i+1} is invalid or has the wrong dimensions!\")\n",
        "\n",
        "        # save video frame\n",
        "        # video_writer.write(annotated_frame)\n",
        "\n",
        "    # Print how long the processing for this frame took\n",
        "    # print(f\"Frame {i+1} processed in {time.time() - start_time:.4f} seconds\")\n",
        "\n",
        "# close output video\n",
        "video_writer.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkVHMbSWKSOa"
      },
      "source": [
        "# Process and Save Results to Join"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ow4-v7_bKXtL"
      },
      "outputs": [],
      "source": [
        "from typing import Union\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "def create_parent_dir(file_path: str) -> None:\n",
        "    file_directory = os.path.dirname(os.path.abspath(file_path))\n",
        "    os.makedirs(file_directory, exist_ok=True)\n",
        "\n",
        "\n",
        "def dump_json_file(file_path: str, content: Union[list, dict], **kwargs) -> None:\n",
        "    create_parent_dir(file_path=file_path)\n",
        "    with open(file_path, \"w\") as file:\n",
        "        json.dump(content, file, **kwargs)\n",
        "\n",
        "def dump_excel_file(file_path: str, content: list) -> None:\n",
        "    create_parent_dir(file_path=file_path)\n",
        "\n",
        "    # Convert list of dictionaries into a pandas DataFrame\n",
        "    df = pd.DataFrame(content)\n",
        "\n",
        "    # Dump data into an Excel file\n",
        "    df.to_excel(file_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOOjTs_49ewU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "# Define colors\n",
        "KEYPOINT_COLOR = (255, 0, 0)  # Blue for keypoints\n",
        "CONNECTION_COLOR = (0, 255, 0)  # Green for connections\n",
        "ANGLE_COLOR_SMALL = (0, 0, 255)  # Red for small angles\n",
        "ANGLE_COLOR_LARGE = (0, 255, 0)  # Green for larger angles\n",
        "\n",
        "\n",
        "CONNECTIONS = [(8, 6), (6, 12)]\n",
        "\n",
        "\n",
        "def angle(kpts,p1,p2,p3):\n",
        "    coord = []\n",
        "    no_kpt = len(kpts)//3\n",
        "    for i in range(no_kpt):\n",
        "      cx,cy = kpts[3*i], kpts[3*i +1]\n",
        "      conf = kpts[3*i +2]\n",
        "      coord.append([i,cx,cy,conf])\n",
        "\n",
        "    points = (p1,p2,p3)\n",
        "\n",
        "\n",
        "    x1,y1 = coord[p1][1:3]\n",
        "    x2,y2 = coord[p2][1:3]\n",
        "    x3,y3 = coord[p3][1:3]\n",
        "\n",
        "    angle = math.degrees(math.atan2(y3-y2,x3-x2)-math.atan2(y1-y2,x1-x2))\n",
        "\n",
        "    if angle < 0:\n",
        "      angle += 360\n",
        "\n",
        "    if angle > 180:\n",
        "      angle = 360 - angle\n",
        "\n",
        "    return int(angle)\n",
        "\n",
        "\n",
        "def draw_angle_on_frame(frame, angle, position):\n",
        "    # font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    # cv2.putText(frame, f\"Right Knee Angle: {angle} degrees\", position, font, 0.6, (0, 255, 0), 2)\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 1\n",
        "    font_thickness = 2\n",
        "    text_color = (0, 255, 0)  # Green text\n",
        "    border_color = (0, 0, 255)  # Blue border\n",
        "    border_thickness = 2\n",
        "    padding = 5  # Padding around the text inside the border\n",
        "\n",
        "    # Create the text string\n",
        "    text = f\"Right Shoulder Angle: {angle} degrees\"\n",
        "\n",
        "    # Get the size of the text\n",
        "    text_size = cv2.getTextSize(text, font, font_scale, font_thickness)\n",
        "    text_width, text_height = text_size[0]\n",
        "    baseline = text_size[1]\n",
        "\n",
        "    # Define the top-left and bottom-right corners of the border\n",
        "    top_left = (position[0] - padding, position[1] - text_height - baseline - padding)\n",
        "    bottom_right = (position[0] + text_width + padding, position[1] + baseline)\n",
        "\n",
        "    # Draw the rectangle border\n",
        "    cv2.rectangle(frame, top_left, bottom_right, border_color, border_thickness)\n",
        "\n",
        "    # Position of the text inside the border\n",
        "    text_position = (position[0], position[1] - baseline)\n",
        "\n",
        "    # Draw the text\n",
        "    cv2.putText(frame, text, text_position, font, font_scale, text_color, font_thickness)\n",
        "\n",
        "\n",
        "# function for detecting foot contacts\n",
        "def detect_foot_contact(pose_output, prev_y_positions, prev_contact_times, fps, frame_number):\n",
        "    \"\"\"\n",
        "    Detects the timestamps when the foot makes contact with the ground and calculates step duration.\n",
        "\n",
        "    Args:\n",
        "        pose_output (np.ndarray): Pose keypoints output from YOLOv7.\n",
        "        prev_y_positions (dict): Stores previous y-coordinates of ankles.\n",
        "        prev_contact_times (dict): Stores previous timestamps for each foot.\n",
        "        fps (float): Frames per second of the video.\n",
        "        frame_number (int): Current frame index.\n",
        "\n",
        "    Returns:\n",
        "        contact_events (list): List of (foot, timestamp, step_duration) tuples.\n",
        "        prev_y_positions (dict): Updated y-coordinate values of ankles.\n",
        "        prev_contact_times (dict): Updated last contact times.\n",
        "    \"\"\"\n",
        "    contact_events = []\n",
        "\n",
        "    if pose_output.shape[0] == 0:\n",
        "        return contact_events, prev_y_positions, prev_contact_times\n",
        "\n",
        "    for idx in range(pose_output.shape[0]):\n",
        "        keypoints = pose_output[idx, 7:]\n",
        "\n",
        "        left_ankle_y = keypoints[15 * 3 + 1]  # y-coordinate of the left ankle\n",
        "        right_ankle_y = keypoints[16 * 3 + 1] # y-coordinate of the right ankle\n",
        "\n",
        "        # Check for contact: if y-coordinate stabilize/has no downward movements\n",
        "        for foot, ankle_y in [(\"left\", left_ankle_y), (\"right\", right_ankle_y)]:\n",
        "            if foot in prev_y_positions:\n",
        "                prev_y = prev_y_positions[foot]\n",
        "                velocity = ankle_y - prev_y   # Change in y (negative values means moving down)\n",
        "                print(velocity)\n",
        "\n",
        "                if abs(velocity) < 2:\n",
        "                    timestamp = frame_number / fps\n",
        "                    step_duration = timestamp - prev_contact_times[foot] if prev_contact_times[foot] is not None else None\n",
        "\n",
        "                    contact_events.append((foot, timestamp, step_duration))\n",
        "                    print(f\"{foot.capitalize()} foot contact at {timestamp:.2f} sec, Step duration: {step_duration:.2f} sec\" if step_duration else f\"{foot.capitalize()} foot contact at {timestamp:.2f} sec\")\n",
        "\n",
        "                    prev_contact_times[foot] = timestamp\n",
        "\n",
        "            prev_y_positions[foot] = ankle_y\n",
        "\n",
        "    return contact_events, prev_y_positions, prev_contact_times\n",
        "\n",
        "\n",
        "def process_and_dump(source_video_path: str, target_json_path: str, target_excel_path:str, target_video_path: str, angle_plot_path: str, task_output_path: str) -> None:\n",
        "    frame_iterator = iter(generate_frames(video_file=source_video_path))\n",
        "    total = get_frame_count(source_video_path)\n",
        "    entries = []\n",
        "    angle_changes = []\n",
        "    shoulder_angle = []\n",
        "    contact_events = []\n",
        "\n",
        "    pre_y_positions = {\"left\": None, \"right\": None}\n",
        "    pre_contact_times = {\"left\": None, \"right\": None}\n",
        "\n",
        "    # Keypoint indices for specific landmarks\n",
        "    SHOULDER_INDEX = 8\n",
        "    ELBOW_INDEX = 6\n",
        "    WRIST_INDEX = 12\n",
        "\n",
        "    # Open video for writing\n",
        "    cap = cv2.VideoCapture(source_video_path)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'H264')\n",
        "        # fourcc = cv2.VideoWriter_fourcc(*'H264')  # H264 is widely supported\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frame_width = int(cap.get(3))\n",
        "    frame_height = int(cap.get(4))\n",
        "    out = cv2.VideoWriter(target_video_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    frame_number = 0\n",
        "\n",
        "    for frame in tqdm(frame_iterator, total=total):\n",
        "        image_size = frame.shape[:2]\n",
        "\n",
        "        #detection\n",
        "        detection_pre_processed_frame = detection_pre_process_frame(frame=frame, device=device)\n",
        "        detection_scaled_image_size = tuple(detection_pre_processed_frame.size())[2:]\n",
        "\n",
        "        detection_output = detection_model(detection_pre_processed_frame)[0].detach().cpu()\n",
        "        detection_output = detection_post_process_output(\n",
        "            output=detection_output,\n",
        "            confidence_trashold=CONFIDENCE_TRESHOLD,\n",
        "            iou_trashold=IOU_TRESHOLD,\n",
        "            image_size=image_size,\n",
        "            scaled_image_size=detection_scaled_image_size\n",
        "        )\n",
        "\n",
        "        # pose\n",
        "        pose_pre_processed_frame = pose_pre_process_frame(frame=frame, device=device)\n",
        "        pose_scaled_image_size = tuple(pose_pre_processed_frame.size())[2:]\n",
        "\n",
        "        pose_output = pose_model(pose_pre_processed_frame)[0].detach().cpu()\n",
        "        pose_output1 = pose_model(pose_pre_processed_frame)[0].detach().cpu()\n",
        "        pose_output = pose_post_process_output(\n",
        "            output=pose_output,\n",
        "            confidence_trashold=CONFIDENCE_TRESHOLD,\n",
        "            iou_trashold=IOU_TRESHOLD,\n",
        "            image_size=image_size,\n",
        "            scaled_image_size=pose_scaled_image_size\n",
        "        )\n",
        "\n",
        "        pose_output1 = non_max_suppression_kpt(\n",
        "            pose_output1,\n",
        "            0.25,\n",
        "            0.65,\n",
        "            nc=pose_model.yaml['nc'],\n",
        "            nkpt=pose_model.yaml['nkpt'],\n",
        "            kpt_label=True\n",
        "        )\n",
        "        pose_output1 = output_to_keypoint(pose_output1)\n",
        "\n",
        "        for idx in range(pose_output1.shape[0]):\n",
        "          kpts=pose_output1[idx,7:].T\n",
        "          elbow_angle=angle(kpts, 8, 6, 12)\n",
        "\n",
        "          # Draw the angle on the frame\n",
        "          draw_angle_on_frame(frame, elbow_angle, (50, 50))\n",
        "\n",
        "          # Draw keypoints, connections, and the angle arc at the elbow\n",
        "          # draw_keypoints_and_connections(frame, kpts)\n",
        "          elbow_position = (int(kpts[3 * ELBOW_INDEX]), int(kpts[3 * ELBOW_INDEX + 1]))\n",
        "          # draw_angle_arc(frame, elbow_angle, elbow_position)\n",
        "\n",
        "          # print(f\"Elbow Angle (Frame {frame_number}): {elbow_angle:.2f} degrees\")\n",
        "\n",
        "          # Save angle and corresponding time for plotting\n",
        "          angle_changes.append((frame_number / fps, elbow_angle))\n",
        "          shoulder_angle.append((frame_number, elbow_angle))\n",
        "\n",
        "        # Save frame to video\n",
        "        out.write(frame)\n",
        "\n",
        "        entry = {\n",
        "            \"detection\": detection_output.tolist(),\n",
        "            \"pose\": pose_output.tolist()\n",
        "        }\n",
        "        entries.append(entry)\n",
        "\n",
        "        # Detect foot contact events and step durations\n",
        "        frame_contact_times, prev_y_positions, prev_contact_times = detect_foot_contact(\n",
        "            pose_output, prev_y_positions, prev_contact_times, fps, framae_number\n",
        "        )\n",
        "        contact_events.extends(frame_contact_times)\n",
        "\n",
        "        frame_number += 1\n",
        "\n",
        "    # Release the video writer and video capture\n",
        "    out.release()\n",
        "    cap.release()\n",
        "\n",
        "    # Save to JSON\n",
        "    dump_json_file(file_path=target_json_path, content=entries)\n",
        "\n",
        "    # Save to Excel\n",
        "    # df = pd.DataFrame(entries)\n",
        "    # create_parent_dir(file_path=target_excel_path)\n",
        "    # df.to_excel(target_excel_path, index=False)\n",
        "    dump_excel_file(file_path=target_excel_path, content=entries)\n",
        "\n",
        "    # Save frame angles to Excel\n",
        "    angle_df = pd.DataFrame(shoulder_angle, columns=[\"Frame\", \"Right Shoulder Angle (degrees)\"])\n",
        "    angle_df.to_excel(target_excel_path, index=False)\n",
        "\n",
        "    # Plotting the angle changes over time\n",
        "    times, angles = zip(*angle_changes)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(times, angles, label=\"Right Shoulder Angle\", color='b')\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Right Shoulder Angle Changes Over Time\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    # plt.show()\n",
        "\n",
        "    # Save the plot to the specified path\n",
        "    plt.savefig(angle_plot_path)\n",
        "    plt.close()\n",
        "\n",
        "    # Step durations\n",
        "    df = pd.DataFrame(contact_events, columns=[\"Foot\", \"Timestamp (s)\", \"Step Duration\"])\n",
        "    df.to_excel(f\"{target_excel_path}/steps_duration.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aelc_vC-Hiz"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28045089d4af44f2b1acac3bc17d78b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'openpyxl'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[161], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mprocess_and_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSOURCE_VIDEO_A_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mHOME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/output/pose-estimation-synchronised-sample-a.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mHOME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/output/right-shoulder-angle-a.xlsx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mHOME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/output/right-shoulder-angles-sample-a.mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mHOME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/output/right-shoulder-angles-sample-a.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mHOME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/output\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# process_and_dump(SOURCE_VIDEO_B_PATH, f\"{HOME}/output/pose-estimation-synchronised-sample-b.json\", f\"{HOME}/output/right-shoulder-angle-b.xlsx\", f\"{HOME}/output/right-shoulder-angles-sample-b.mp4\", f\"{HOME}/output/right-shoulder-angles-sample-b.png\", f\"{HOME}/output\")\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# process_and_dump(SOURCE_VIDEO_C_PATH, f\"{HOME}/output/pose-estimation-synchronised-sample-c.json\", f\"{HOME}/output/right-shoulder-angle-c.xlsx\", f\"{HOME}/output/right-shoulder-angles-sample-c.mp4\", f\"{HOME}/output/right-shoulder-angles-sample-c.png\", f\"{HOME}/output\")\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[160], line 237\u001b[0m, in \u001b[0;36mprocess_and_dump\u001b[1;34m(source_video_path, target_json_path, target_excel_path, target_video_path, angle_plot_path, task_output_path)\u001b[0m\n\u001b[0;32m    231\u001b[0m dump_json_file(file_path\u001b[38;5;241m=\u001b[39mtarget_json_path, content\u001b[38;5;241m=\u001b[39mentries)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;66;03m# Save to Excel\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame(entries)\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# create_parent_dir(file_path=target_excel_path)\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;66;03m# df.to_excel(target_excel_path, index=False)\u001b[39;00m\n\u001b[1;32m--> 237\u001b[0m \u001b[43mdump_excel_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_excel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mentries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# Save frame angles to Excel\u001b[39;00m\n\u001b[0;32m    240\u001b[0m angle_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(shoulder_angle, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRight Shoulder Angle (degrees)\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
            "Cell \u001b[1;32mIn[158], line 23\u001b[0m, in \u001b[0;36mdump_excel_file\u001b[1;34m(file_path, content)\u001b[0m\n\u001b[0;32m     20\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(content)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Dump data into an Excel file\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py:2417\u001b[0m, in \u001b[0;36mNDFrame.to_excel\u001b[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   2404\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexcel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[0;32m   2406\u001b[0m formatter \u001b[38;5;241m=\u001b[39m ExcelFormatter(\n\u001b[0;32m   2407\u001b[0m     df,\n\u001b[0;32m   2408\u001b[0m     na_rep\u001b[38;5;241m=\u001b[39mna_rep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2415\u001b[0m     inf_rep\u001b[38;5;241m=\u001b[39minf_rep,\n\u001b[0;32m   2416\u001b[0m )\n\u001b[1;32m-> 2417\u001b[0m \u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2418\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexcel_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2419\u001b[0m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartcol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2426\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\io\\formats\\excel.py:943\u001b[0m, in \u001b[0;36mExcelFormatter.write\u001b[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m    941\u001b[0m     need_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 943\u001b[0m     writer \u001b[38;5;241m=\u001b[39m \u001b[43mExcelWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    949\u001b[0m     need_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:57\u001b[0m, in \u001b[0;36mOpenpyxlWriter.__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     46\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m WriteExcelBuffer \u001b[38;5;241m|\u001b[39m ExcelWriter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Use the openpyxl module as the Excel writer.\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenpyxl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworkbook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[0;32m     59\u001b[0m     engine_kwargs \u001b[38;5;241m=\u001b[39m combine_kwargs(engine_kwargs, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     62\u001b[0m         path,\n\u001b[0;32m     63\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m         engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[0;32m     67\u001b[0m     )\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openpyxl'"
          ]
        }
      ],
      "source": [
        "process_and_dump(SOURCE_VIDEO_A_PATH, f\"{HOME}/output/pose-estimation-synchronised-sample-a.json\", f\"{HOME}/output/right-shoulder-angle-a.xlsx\", f\"{HOME}/output/right-shoulder-angles-sample-a.mp4\", f\"{HOME}/output/right-shoulder-angles-sample-a.png\", f\"{HOME}/output\")\n",
        "# process_and_dump(SOURCE_VIDEO_B_PATH, f\"{HOME}/output/pose-estimation-synchronised-sample-b.json\", f\"{HOME}/output/right-shoulder-angle-b.xlsx\", f\"{HOME}/output/right-shoulder-angles-sample-b.mp4\", f\"{HOME}/output/right-shoulder-angles-sample-b.png\", f\"{HOME}/output\")\n",
        "# process_and_dump(SOURCE_VIDEO_C_PATH, f\"{HOME}/output/pose-estimation-synchronised-sample-c.json\", f\"{HOME}/output/right-shoulder-angle-c.xlsx\", f\"{HOME}/output/right-shoulder-angles-sample-c.mp4\", f\"{HOME}/output/right-shoulder-angles-sample-c.png\", f\"{HOME}/output\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mhbnGsVio21"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('..')\n",
        "%cd content\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhUpYQlxdWUj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "excel_file_a = f\"{HOME}/output/right-shoulder-angle-a.xlsx\"\n",
        "excel_file_b = f\"{HOME}/output/right-shoulder-angle-b.xlsx\"\n",
        "excel_file_c = f\"{HOME}/output/right-shoulder-angle-c.xlsx\"\n",
        "\n",
        "file_paths = [excel_file_a, excel_file_b, excel_file_c]\n",
        "colors = ['blue', 'green', 'red']  # Colors for each plot\n",
        "labels = ['Left Camera', 'Middle Camera', 'Right Camera']  # Labels for the legend\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for file_path, color, label in zip(file_paths, colors, labels):\n",
        "    # Read the Excel file\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "    # Plot Frame vs. Angle\n",
        "    plt.plot(df['Frame'], df['Right Shoulder Angle (degrees)'], color=color, label=label)\n",
        "\n",
        "# Label the axes and add a title\n",
        "plt.xlabel('Frame')\n",
        "plt.ylabel('Right Shoulder Angle (degrees)')\n",
        "plt.title('Right Shoulder Angle Changes Over Time')\n",
        "\n",
        "# Add a legend to distinguish between the files\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "# plt.show()\n",
        "\n",
        "# Save the plot as a PNG file\n",
        "plt.savefig(f'{HOME}/output/right_shoulder_angle_changes_over_time.png', format='png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSzbp6HO91QS"
      },
      "source": [
        "# **Animation Creation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq7y1rb-9-MH"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXmjxA_Q-m3_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('..')\n",
        "%cd content\n",
        "# print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zt6Tp79A-Ew6"
      },
      "outputs": [],
      "source": [
        "FRAME_WIDTH = 1920\n",
        "FRAME_HEIGHT = 1080"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvVwauiy-LRF"
      },
      "outputs": [],
      "source": [
        "\n",
        "VIEW_X_MIN = - 500\n",
        "VIEW_X_MAX = 500\n",
        "VIEW_Y_MIN = - 500\n",
        "VIEW_Y_MAX = 500\n",
        "VIEW_Z_MIN = 0\n",
        "VIEW_Z_MAX = 1000\n",
        "\n",
        "POSE_ANCHORS = [\n",
        "    [0,1],\n",
        "    [0,2],\n",
        "    [1,3],\n",
        "    [2,4],\n",
        "    [5,6],\n",
        "    [5,7],\n",
        "    [6,8],\n",
        "    [7,9],\n",
        "    [8,10],\n",
        "    [5,11],\n",
        "    [6,12],\n",
        "    [11,12],\n",
        "    [11,13],\n",
        "    [12,14],\n",
        "    [13,15],\n",
        "    [14,16]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPH87PnU-wXE"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import json\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class FrameData:\n",
        "    pose: np.ndarray\n",
        "    detection: np.ndarray\n",
        "\n",
        "\n",
        "def load_json(path: str) -> dict:\n",
        "    with open(path) as f:\n",
        "        contents = f.read()\n",
        "        return json.loads(contents)\n",
        "\n",
        "\n",
        "def load_extracted_data(path: str) -> List[FrameData]:\n",
        "    raw = load_json(path)\n",
        "    return [\n",
        "        FrameData(\n",
        "            pose=entry['pose'],\n",
        "            detection=entry['detection']\n",
        "        )\n",
        "        for entry\n",
        "        in raw\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3H4i975_o4U"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY6clvhK-tNV"
      },
      "outputs": [],
      "source": [
        "EXTRACTED_DATA_A_PATH = f\"{HOME}/output/pose-estimation-synchronised-sample-a.json\"\n",
        "EXTRACTED_DATA_B_PATH = f\"{HOME}/output/pose-estimation-synchronised-sample-b.json\"\n",
        "EXTRACTED_DATA_C_PATH = f\"{HOME}/output/pose-estimation-synchronised-sample-c.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3rbQYA6-0Iw"
      },
      "outputs": [],
      "source": [
        "extracted_data_a = load_extracted_data(EXTRACTED_DATA_A_PATH)\n",
        "extracted_data_b = load_extracted_data(EXTRACTED_DATA_B_PATH)\n",
        "extracted_data_c = load_extracted_data(EXTRACTED_DATA_C_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33ZkyzzC-2vi"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, List, Tuple\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Pose:\n",
        "    x: np.ndarray\n",
        "    y: np.ndarray\n",
        "    confidence: np.ndarray\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, data: List[float]) -> Pose:\n",
        "        x, y, confidence = [], [], []\n",
        "        for i in range(17):\n",
        "            x.append(data[7 + i * 3])\n",
        "            y.append(data[7 + i * 3 + 1])\n",
        "            confidence.append(data[7 + i * 3 + 2])\n",
        "        return Pose(\n",
        "            x=np.array(x),\n",
        "            y=np.array(y),\n",
        "            confidence=np.array(confidence)\n",
        "        )\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Pose3D:\n",
        "    x: np.ndarray\n",
        "    y: np.ndarray\n",
        "    z: np.ndarray\n",
        "\n",
        "    @classmethod\n",
        "    def from2D(cls, pose_a: Pose, pose_b: Pose, pose_c: Pose) -> Pose3D:\n",
        "        return Pose3D(\n",
        "            x=pose_a.x,\n",
        "            y=pose_b.x,\n",
        "            z=(pose_a.y + pose_b.y + pose_c.y) / 3\n",
        "        )\n",
        "\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional\n",
        "\n",
        "# Point class represents a point with x and y coordinates\n",
        "@dataclass\n",
        "class Point:\n",
        "    x: float\n",
        "    y: float\n",
        "\n",
        "# Detection class represents a bounding box around a detected object\n",
        "@dataclass\n",
        "class Detection:\n",
        "    x_min: float\n",
        "    y_min: float\n",
        "    x_max: float\n",
        "    y_max: float\n",
        "    confidence: float\n",
        "    class_id: int\n",
        "\n",
        "    @property\n",
        "    def width(self) -> float:\n",
        "        return self.x_max - self.x_min\n",
        "\n",
        "    @property\n",
        "    def height(self) -> float:\n",
        "        return self.y_max - self.y_min\n",
        "\n",
        "    @property\n",
        "    def center(self) -> Point:\n",
        "        return Point(\n",
        "            x=(self.x_min + self.x_max) / 2,\n",
        "            y=(self.y_min + self.y_max) / 2\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, data: List[float]) -> 'Detection':\n",
        "        return Detection(\n",
        "            x_min=float(data[0]),\n",
        "            y_min=float(data[1]),\n",
        "            x_max=float(data[2]),\n",
        "            y_max=float(data[3]),\n",
        "            confidence=float(data[4]),\n",
        "            class_id=int(data[5])\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def filter(cls, detections: List['Detection'], person_class_id: int = 0) -> Optional['Detection']:\n",
        "        # Filter detections for the person class (default class ID is 0)\n",
        "        filtered_detections = [\n",
        "            detection for detection in detections\n",
        "            if detection.class_id == person_class_id\n",
        "        ]\n",
        "        # Return the first detection of a person if one exists\n",
        "        return filtered_detections[0] if filtered_detections else None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuZs5YAn_Hf0"
      },
      "outputs": [],
      "source": [
        "detections = [Detection.load(detection) for detection in extracted_data_a[0].detection]\n",
        "detection_person = Detection.filter(detections)\n",
        "\n",
        "# Check if a person was detected\n",
        "if detection_person is not None:\n",
        "    pose_a = Pose.load(extracted_data_a[0].pose[0])  # Load the person's pose data\n",
        "else:\n",
        "    pose_a = None  # Handle the case where no person is detected\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehx8w7SJ_J2m"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check if pose_a is valid (i.e., person detected)\n",
        "if pose_a is not None:\n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "    plt.scatter(pose_a.x, pose_a.y, color=\"red\")\n",
        "    plt.xlim([0, FRAME_WIDTH])\n",
        "    plt.ylim([0, FRAME_HEIGHT])\n",
        "    plt.gca().invert_yaxis()  # Invert y-axis if needed to match the video coordinates\n",
        "    plt.show()\n",
        "else:\n",
        "    # Show a completely blank page\n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "    plt.xlim([0, FRAME_WIDTH])\n",
        "    plt.ylim([0, FRAME_HEIGHT])\n",
        "    plt.gca().invert_yaxis()  # Match the y-axis orientation with the video\n",
        "    plt.show()  # Show blank plot without text or scatter points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxOKcY51_MUm"
      },
      "outputs": [],
      "source": [
        "if detection_person is not None:\n",
        "    pose_b = Pose.load(extracted_data_b[0].pose[0])  # Load the person's pose data\n",
        "else:\n",
        "    pose_b = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXN6l5zo_boI"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check if pose_a is valid (i.e., person detected)\n",
        "if pose_b is not None:\n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "    plt.scatter(pose_b.x, pose_b.y, color=\"red\")\n",
        "    plt.xlim([0, FRAME_WIDTH])\n",
        "    plt.ylim([0, FRAME_HEIGHT])\n",
        "    plt.gca().invert_yaxis()  # Invert y-axis if needed to match the video coordinates\n",
        "    plt.show()\n",
        "else:\n",
        "    # Show a completely blank page\n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "    plt.xlim([0, FRAME_WIDTH])\n",
        "    plt.ylim([0, FRAME_HEIGHT])\n",
        "    plt.gca().invert_yaxis()  # Match the y-axis orientation with the video\n",
        "    plt.show()  # Show blank plot without text or scatter points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFvEuBcr9zOj"
      },
      "outputs": [],
      "source": [
        "if detection_person is not None:\n",
        "    pose_c = Pose.load(extracted_data_c[0].pose[0])  # Load the person's pose data\n",
        "else:\n",
        "    pose_c = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGQMFPL795Z2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check if pose_a is valid (i.e., person detected)\n",
        "if pose_c is not None:\n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "    plt.scatter(pose_c.x, pose_c.y, color=\"red\")\n",
        "    plt.xlim([0, FRAME_WIDTH])\n",
        "    plt.ylim([0, FRAME_HEIGHT])\n",
        "    plt.gca().invert_yaxis()  # Invert y-axis if needed to match the video coordinates\n",
        "    plt.show()\n",
        "else:\n",
        "    # Show a completely blank page\n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "    plt.xlim([0, FRAME_WIDTH])\n",
        "    plt.ylim([0, FRAME_HEIGHT])\n",
        "    plt.gca().invert_yaxis()  # Match the y-axis orientation with the video\n",
        "    plt.show()  # Show blank plot without text or scatter points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhpIBk_N_eHC"
      },
      "outputs": [],
      "source": [
        "detections = [Detection.load(detection) for detection in extracted_data_a[0].detection]\n",
        "detection_a = Detection.filter(detections, 0)\n",
        "pose_a = Pose.load(extracted_data_a[0].pose[0])\n",
        "BASELINE_HEIGHT_A = detection_a.height\n",
        "BASELINE_VERTICAL_OFFSET_A = detection_a.y_max - pose_a.y.max()\n",
        "print(\"BASELINE_HEIGHT_A\", BASELINE_HEIGHT_A)\n",
        "print(\"BASELINE_VERTICAL_OFFSET_A\", BASELINE_VERTICAL_OFFSET_A)\n",
        "\n",
        "detections = [Detection.load(detection) for detection in extracted_data_b[0].detection]\n",
        "detection_b = Detection.filter(detections, 0)\n",
        "pose_b = Pose.load(extracted_data_b[0].pose[0])\n",
        "BASELINE_VERTICAL_OFFSET_B = detection_b.y_max - pose_b.y.max()\n",
        "BASELINE_HEIGHT_B = detection_b.height\n",
        "print(\"BASELINE_HEIGHT_B\", BASELINE_HEIGHT_B)\n",
        "print(\"BASELINE_VERTICAL_OFFSET_B\", BASELINE_VERTICAL_OFFSET_B)\n",
        "\n",
        "detections = [Detection.load(detection) for detection in extracted_data_c[0].detection]\n",
        "detection_c = Detection.filter(detections, 0)\n",
        "pose_c = Pose.load(extracted_data_c[0].pose[0])\n",
        "BASELINE_VERTICAL_OFFSET_C = detection_c.y_max - pose_c.y.max()\n",
        "BASELINE_HEIGHT_C = detection_c.height\n",
        "print(\"BASELINE_HEIGHT_C\", BASELINE_HEIGHT_C)\n",
        "print(\"BASELINE_VERTICAL_OFFSET_C\", BASELINE_VERTICAL_OFFSET_C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLlRDtPS_g7d"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Tuple\n",
        "\n",
        "\n",
        "def calibrate(\n",
        "    data: FrameData,\n",
        "    frame_height: int,\n",
        "    baseline_pose_height: float,\n",
        "    baseline_vertical_offset: float\n",
        ") -> Optional[Tuple[Pose, Point]]:\n",
        "    detections = [Detection.load(detection) for detection in data.detection]\n",
        "    detection_person = Detection.filter(detections, 0)\n",
        "    # detection_ball = Detection.filter(detections, 32)\n",
        "\n",
        "    if detection_person is None:\n",
        "        return None\n",
        "    # if detection_ball is None:\n",
        "    #     return None\n",
        "    if len(data.pose) != 1:\n",
        "        return None\n",
        "\n",
        "    # ball_x, ball_y = detection_ball.center.int_xy_tuple\n",
        "\n",
        "    pose = Pose.load(data.pose[0])\n",
        "    pose.y = frame_height - pose.y\n",
        "    # ball_y = frame_height - ball_y\n",
        "\n",
        "    x_shift = (pose.x.max() + pose.x.min()) / 2\n",
        "    y_shift = pose.y.min() - baseline_vertical_offset\n",
        "\n",
        "    pose.x = pose.x - x_shift\n",
        "    # ball_x = ball_x - x_shift\n",
        "    pose.y = (pose.y - y_shift) * 1000 / baseline_pose_height\n",
        "    # ball_y = (ball_y - y_shift) * 1000 / baseline_pose_height\n",
        "    return pose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7mg0iDD_kXe"
      },
      "outputs": [],
      "source": [
        "pose_a = calibrate(extracted_data_a[0], FRAME_HEIGHT, BASELINE_HEIGHT_A, BASELINE_VERTICAL_OFFSET_A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gCmim3Z_mX1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(12, 12))\n",
        "plt.scatter(pose_a.x, pose_a.y, color=\"red\")\n",
        "# plt.scatter([point_a.x], [point_a.y], color=\"blue\")\n",
        "plt.xlim([VIEW_X_MIN, VIEW_X_MAX])\n",
        "plt.ylim([VIEW_Z_MIN, VIEW_Z_MAX])\n",
        "ax = plt.gca()\n",
        "ax.set_aspect('equal', adjustable='box')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34TOY95T_oWv"
      },
      "outputs": [],
      "source": [
        "pose_b = calibrate(extracted_data_b[0], FRAME_HEIGHT, BASELINE_HEIGHT_B, BASELINE_VERTICAL_OFFSET_B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9w6f-GO_qgy"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(12, 12))\n",
        "plt.scatter(pose_b.x, pose_b.y, color=\"red\")\n",
        "# plt.scatter([point_b.x], [point_b.y], color=\"blue\")\n",
        "plt.xlim([VIEW_X_MIN, VIEW_X_MAX])\n",
        "plt.ylim([VIEW_Z_MIN, VIEW_Z_MAX])\n",
        "ax = plt.gca()\n",
        "ax.set_aspect('equal', adjustable='box')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcbmwpb3-JF2"
      },
      "outputs": [],
      "source": [
        "pose_c = calibrate(extracted_data_c[0], FRAME_HEIGHT, BASELINE_HEIGHT_C, BASELINE_VERTICAL_OFFSET_C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUiqBoPy-eqt"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(12, 12))\n",
        "plt.scatter(pose_c.x, pose_c.y, color=\"red\")\n",
        "# plt.scatter([point_b.x], [point_b.y], color=\"blue\")\n",
        "plt.xlim([VIEW_X_MIN, VIEW_X_MAX])\n",
        "plt.ylim([VIEW_Z_MIN, VIEW_Z_MAX])\n",
        "ax = plt.gca()\n",
        "ax.set_aspect('equal', adjustable='box')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKvTBrEA_wPM"
      },
      "outputs": [],
      "source": [
        "pose_a = calibrate(extracted_data_a[0], FRAME_HEIGHT, BASELINE_HEIGHT_A, BASELINE_VERTICAL_OFFSET_A)\n",
        "pose_b = calibrate(extracted_data_b[0], FRAME_HEIGHT, BASELINE_HEIGHT_B, BASELINE_VERTICAL_OFFSET_B)\n",
        "pose_c = calibrate(extracted_data_c[0], FRAME_HEIGHT, BASELINE_HEIGHT_C, BASELINE_VERTICAL_OFFSET_C)\n",
        "\n",
        "pose3d = Pose3D.from2D(pose_a=pose_a, pose_b=pose_b, pose_c=pose_c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFu4TCCO_2sX"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, List\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "\n",
        "class Trace:\n",
        "\n",
        "    def __init__(self, history: int = 10):\n",
        "        self.x = deque(maxlen=history)\n",
        "        self.y = deque(maxlen=history)\n",
        "        self.z = deque(maxlen=history)\n",
        "\n",
        "    def append(self, x: float, y: float, z: float):\n",
        "        self.x.append(x)\n",
        "        self.y.append(y)\n",
        "        self.z.append(z)\n",
        "\n",
        "    def get_state(self) -> Tuple[List[float], List[float], List[float]]:\n",
        "        return list(self.x), list(self.y), list(self.z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hq2UHGhvbcvU"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def calculate_angle_3d(a, b, c):\n",
        "\n",
        "    # Calculate the angle between three points in 3D space.\n",
        "\n",
        "    # Parameters:\n",
        "    # a, b, c: 3D points as numpy arrays where b is the vertex (shoulder),\n",
        "    #          a is one end (elbow), and c is the other end (hip).\n",
        "\n",
        "    # Returns:\n",
        "    # angle: The angle in degrees.\n",
        "\n",
        "    a = np.array(a)  # First point (e.g., elbow)\n",
        "    b = np.array(b)  # Middle point (e.g., shoulder)\n",
        "    c = np.array(c)  # End point (e.g., hip)\n",
        "\n",
        "    # Vector BA and BC\n",
        "    ba = a - b\n",
        "    bc = c - b\n",
        "\n",
        "    # Calculate the cosine of the angle using the dot product formula\n",
        "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
        "\n",
        "    # Clip the cosine value to the range [-1, 1] to avoid numerical errors\n",
        "    cosine_angle = np.clip(cosine_angle, -1.0, 1.0)\n",
        "\n",
        "    # Calculate the angle in radians and then convert to degrees\n",
        "    angle = np.arccos(cosine_angle) * (180.0 / np.pi)\n",
        "\n",
        "    return angle\n",
        "\n",
        "\n",
        "def draw_3d1(\n",
        "    pose3d: Pose3D,\n",
        "    joint_angle: float = 0.0,\n",
        "    angle: int = 0,\n",
        "    save_path: Optional[str] = None,\n",
        "    foot_1_trace: Optional[Trace] = None,\n",
        "    foot_2_trace: Optional[Trace] = None,\n",
        "    angle_point: Optional[str] = \"\",\n",
        ") -> None:\n",
        "\n",
        "    plt.style.use('dark_background')\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 12))\n",
        "    ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "    ax.xaxis.pane.fill = False\n",
        "    ax.yaxis.pane.fill = False\n",
        "    ax.zaxis.pane.fill = False\n",
        "\n",
        "    ax.axes.set_xlim3d(left=VIEW_X_MIN, right=VIEW_X_MAX)\n",
        "    ax.axes.set_ylim3d(bottom=VIEW_Y_MIN, top=VIEW_Y_MAX)\n",
        "    ax.axes.set_zlim3d(bottom=VIEW_Z_MIN, top=VIEW_Z_MAX)\n",
        "\n",
        "    # Plot the pose anchors\n",
        "    for pose_anchor in POSE_ANCHORS:\n",
        "        ax.plot(pose3d.x[pose_anchor], pose3d.y[pose_anchor], pose3d.z[pose_anchor], color=\"#ffffff\", linewidth=5)\n",
        "\n",
        "    # Add the calculated angle as text to the plot\n",
        "    fig.text(0.05, 0.95, f\"{angle_point} Angle: {joint_angle:.2f}°\", color=\"yellow\", fontsize=24,\n",
        "             verticalalignment='top', horizontalalignment='left', backgroundcolor=\"black\")\n",
        "\n",
        "    # Plot foot trace 1 if available\n",
        "    if foot_1_trace is not None:\n",
        "        foot_1_trace_x, foot_1_trace_y, foot_1_trace_z = foot_1_trace.get_state()\n",
        "        ax.plot(foot_1_trace_x, foot_1_trace_y, foot_1_trace_z, color=\"#fecea8\", linewidth=2)\n",
        "\n",
        "    # Plot foot trace 2 if available\n",
        "    if foot_2_trace is not None:\n",
        "        foot_2_trace_x, foot_2_trace_y, foot_2_trace_z = foot_2_trace.get_state()\n",
        "        ax.plot(foot_2_trace_x, foot_2_trace_y, foot_2_trace_z, color=\"#fecea8\", linewidth=2)\n",
        "\n",
        "    # Scatter plot for the pose\n",
        "    ax.scatter(pose3d.x, pose3d.y, pose3d.z, color=\"#ffffff\")\n",
        "\n",
        "    # Set the viewing angle\n",
        "    ax.view_init(30, 45 + angle*2)\n",
        "\n",
        "    # Save or show the plot\n",
        "    if save_path is None:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.savefig(save_path)\n",
        "        plt.close(fig)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q94bOqR4bfxC"
      },
      "outputs": [],
      "source": [
        "draw_3d1(pose3d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fM6SqExfb1_s"
      },
      "outputs": [],
      "source": [
        "pose_a = calibrate(extracted_data_a[63], FRAME_HEIGHT, BASELINE_HEIGHT_A, BASELINE_VERTICAL_OFFSET_A)\n",
        "pose_b = calibrate(extracted_data_b[63], FRAME_HEIGHT, BASELINE_HEIGHT_B, BASELINE_VERTICAL_OFFSET_B)\n",
        "pose_c = calibrate(extracted_data_c[63], FRAME_HEIGHT, BASELINE_HEIGHT_C, BASELINE_VERTICAL_OFFSET_C)\n",
        "\n",
        "pose3d = Pose3D.from2D(pose_a=pose_a, pose_b=pose_b, pose_c=pose_c)\n",
        "\n",
        "draw_3d1(pose3d, 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHUjxyoCcEyN"
      },
      "outputs": [],
      "source": [
        "OUTPUT_DIR = \"frames\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pax4DFEwcFlV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('..')\n",
        "%cd {HOME}\n",
        "!mkdir -p $OUTPUT_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhrnwZ0_cnws"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def save_to_json(frame_index: int, pose3d: Pose3D, json_data: dict):\n",
        "    json_data[frame_index] = {\n",
        "        \"pose\": {\n",
        "            \"x\": pose3d.x.tolist(),\n",
        "            \"y\": pose3d.y.tolist(),\n",
        "            \"z\": pose3d.z.tolist()\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "def save_to_excel(json_data: dict, excel_file_path: str, angle_data: dict):\n",
        "    # Create a list to hold the structured data\n",
        "    data = []\n",
        "\n",
        "    # Loop through the json_data to extract the frame and 3D pose information\n",
        "    for frame_index, frame_data in json_data.items():\n",
        "        # Get the x, y, z lists for the current frame\n",
        "        x_coords = frame_data['pose']['x']\n",
        "        y_coords = frame_data['pose']['y']\n",
        "        z_coords = frame_data['pose']['z']\n",
        "        angle = angle_data.get(frame_index, None)\n",
        "\n",
        "        # Combine all keypoints for this frame into rows (one row per frame)\n",
        "        for kp_idx in range(len(x_coords)):\n",
        "            data.append([frame_index, kp_idx + 1, x_coords[kp_idx], y_coords[kp_idx], z_coords[kp_idx], angle])\n",
        "\n",
        "    # Create a DataFrame with appropriate column names\n",
        "    df = pd.DataFrame(data, columns=['Frame', 'Keypoint', 'X', 'Y', 'Z', 'Elbow_Angle'])\n",
        "\n",
        "    # Save the DataFrame to an Excel file\n",
        "    df.to_excel(excel_file_path, index=False)\n",
        "    print(f\"Excel file saved to {excel_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qxk6z85v-9MI"
      },
      "outputs": [],
      "source": [
        "pip install pandas openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH3J6K9e_BP_"
      },
      "outputs": [],
      "source": [
        "from tqdm import trange\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "head_idx = 0  # Index for head keypoint\n",
        "foot_idx = 15  # Index for foot keypoint (adjust based on your dataset)\n",
        "\n",
        "def calculate_distance(point1, point2):\n",
        "    \"\"\"Calculate Euclidean distance between two 3D points.\"\"\"\n",
        "    return np.sqrt((point2[0] - point1[0]) ** 2 +\n",
        "                   (point2[1] - point1[1]) ** 2 +\n",
        "                   (point2[2] - point1[2]) ** 2)\n",
        "\n",
        "def calculate_pose_anchor_distances(pose3d, anchors):\n",
        "    \"\"\"Calculate distances between keypoint pairs defined in POSE_ANCHORS.\"\"\"\n",
        "    distances = []\n",
        "    for anchor in anchors:\n",
        "        kp1_idx, kp2_idx = anchor\n",
        "        point1 = (pose3d.x[kp1_idx], pose3d.y[kp1_idx], pose3d.z[kp1_idx])\n",
        "        point2 = (pose3d.x[kp2_idx], pose3d.y[kp2_idx], pose3d.z[kp2_idx])\n",
        "        distance = calculate_distance(point1, point2)\n",
        "        distances.append(distance)\n",
        "    return distances\n",
        "\n",
        "def calculate_person_height_in_pixels(pose3d):\n",
        "    \"\"\"Calculate the height of a person in pixels using 3D pose keypoints.\"\"\"\n",
        "    head_point = (pose3d.x[head_idx], pose3d.y[head_idx], pose3d.z[head_idx])\n",
        "    foot_point = (pose3d.x[foot_idx], pose3d.y[foot_idx], pose3d.z[foot_idx])\n",
        "\n",
        "    # Calculate the distance between the head and foot in 3D space (in pixels)\n",
        "    pixel_height = calculate_distance(head_point, foot_point)\n",
        "    return pixel_height\n",
        "\n",
        "# Step 2: Convert pixel height to real height (meters)\n",
        "def calculate_real_height(pixel_height, pixel_to_meter_ratio):\n",
        "    \"\"\"Convert pixel height to real-world height (in meters).\"\"\"\n",
        "    real_height = pixel_height * pixel_to_meter_ratio\n",
        "    return real_height\n",
        "\n",
        "\n",
        "def calculate_angle(a, b, c):\n",
        "    \"\"\"Calculate angle ABC given three points A, B, C.\"\"\"\n",
        "    ab = np.array(b) - np.array(a)\n",
        "    cb = np.array(b) - np.array(c)\n",
        "\n",
        "    # Use the dot product to find the cosine of the angle\n",
        "    cosine_angle = np.dot(ab, cb) / (np.linalg.norm(ab) * np.linalg.norm(cb))\n",
        "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))  # Clip for numerical stability\n",
        "    return np.degrees(angle)  # Convert from radians to degrees\n",
        "\n",
        "\n",
        "# Example usage\n",
        "pose3d = Pose3D.from2D(pose_a=pose_a, pose_b=pose_b, pose_c=pose_c)  # Example pose data\n",
        "pixel_height = calculate_person_height_in_pixels(pose3d)\n",
        "\n",
        "# Assuming a pixel-to-meter ratio, e.g., 0.002 meters per pixel\n",
        "# pixel_to_meter_ratio = object_height_in_meters / object_height_in_pixels\n",
        "pixel_to_meter_ratio = 0.002  # meters per pixel (this should be calculated or calibrated)\n",
        "\n",
        "person_real_height = calculate_real_height(pixel_height, pixel_to_meter_ratio)\n",
        "print(f\"The person's estimated height is: {person_real_height} meters\")\n",
        "\n",
        "def generate_frames(start=0, end=100):\n",
        "    foot_1_trace = Trace(10)\n",
        "    foot_2_trace = Trace(10)\n",
        "    json_data = {}\n",
        "    angle_data = {}\n",
        "\n",
        "    # Initialize an empty DataFrame for storing distances\n",
        "    distance_columns = ['Frame'] + [f'Distance_{i}' for i in range(len(POSE_ANCHORS))]\n",
        "    distance_df = pd.DataFrame(columns=distance_columns)\n",
        "\n",
        "    for i in trange(start, end):\n",
        "        a = calibrate(extracted_data_a[i], FRAME_HEIGHT, BASELINE_HEIGHT_A, BASELINE_VERTICAL_OFFSET_A)\n",
        "        b = calibrate(extracted_data_b[i], FRAME_HEIGHT, BASELINE_HEIGHT_B, BASELINE_VERTICAL_OFFSET_B)\n",
        "        c = calibrate(extracted_data_c[i], FRAME_HEIGHT, BASELINE_HEIGHT_C, BASELINE_VERTICAL_OFFSET_C)\n",
        "\n",
        "        if a is None or b is None:\n",
        "            continue\n",
        "\n",
        "        pose_a = a\n",
        "        pose_b = b\n",
        "        pose_c = c\n",
        "\n",
        "        pose3d = Pose3D.from2D(pose_a=pose_a, pose_b=pose_b, pose_c=pose_c)\n",
        "        foot_1_trace.append(x=pose3d.x[-1], y=pose3d.y[-1], z=pose3d.z[-1])\n",
        "        foot_2_trace.append(x=pose3d.x[-2], y=pose3d.y[-2], z=pose3d.z[-2])\n",
        "\n",
        "        # Calculate distances for the pose keypoints\n",
        "        distances = calculate_pose_anchor_distances(pose3d, POSE_ANCHORS)\n",
        "        new_row = pd.DataFrame([[i] + distances], columns=distance_df.columns)\n",
        "        distance_df = pd.concat([distance_df, new_row], ignore_index=True)\n",
        "\n",
        "        save_to_json(i, pose3d, json_data)\n",
        "\n",
        "        shoulder_idx = 8\n",
        "        elbow_idx = 6\n",
        "        wrist_idx = 12\n",
        "\n",
        "        # Calculate the elbow angles in 2D for each pose\n",
        "        elbow_angle_a = calculate_angle(\n",
        "            (pose_a.x[shoulder_idx], pose_a.y[shoulder_idx]),\n",
        "            (pose_a.x[elbow_idx], pose_a.y[elbow_idx]),\n",
        "            (pose_a.x[wrist_idx], pose_a.y[wrist_idx])\n",
        "        )\n",
        "\n",
        "        elbow_angle_b = calculate_angle(\n",
        "            (pose_b.x[shoulder_idx], pose_b.y[shoulder_idx]),\n",
        "            (pose_b.x[elbow_idx], pose_b.y[elbow_idx]),\n",
        "            (pose_b.x[wrist_idx], pose_b.y[wrist_idx])\n",
        "        )\n",
        "\n",
        "        elbow_angle_c = calculate_angle(\n",
        "            (pose_c.x[shoulder_idx], pose_c.y[shoulder_idx]),\n",
        "            (pose_c.x[elbow_idx], pose_c.y[elbow_idx]),\n",
        "            (pose_c.x[wrist_idx], pose_c.y[wrist_idx])\n",
        "        )\n",
        "\n",
        "        # Calculate the elbow angle in 3D for the reconstructed pose\n",
        "        # elbow_angle_3d = calculate_angle(\n",
        "        #     (pose3d.x[shoulder_idx], pose3d.y[shoulder_idx], pose3d.z[shoulder_idx]),\n",
        "        #     (pose3d.x[elbow_idx], pose3d.y[elbow_idx], pose3d.z[elbow_idx]),\n",
        "        #     (pose3d.x[wrist_idx], pose3d.y[wrist_idx], pose3d.z[wrist_idx])\n",
        "        # )\n",
        "        elbow_angle_3d = calculate_angle_3d(\n",
        "            (pose3d.x[shoulder_idx], pose3d.y[shoulder_idx], pose3d.z[shoulder_idx]),\n",
        "            (pose3d.x[elbow_idx], pose3d.y[elbow_idx], pose3d.z[elbow_idx]),\n",
        "            (pose3d.x[wrist_idx], pose3d.y[wrist_idx], pose3d.z[wrist_idx])\n",
        "        )\n",
        "\n",
        "        # Store all the calculated elbow angles in the angle_data dictionary\n",
        "        angle_data[i] = {\n",
        "            'shoulder_angle_a': elbow_angle_a,\n",
        "            'shoulder_angle_b': elbow_angle_b,\n",
        "            'shoulder_angle_c': elbow_angle_c,\n",
        "            'shoulder_angle_3d': elbow_angle_3d,\n",
        "        }\n",
        "\n",
        "        file_name = f\"file{str(i).zfill(4)}.png\"\n",
        "        # draw_3d1(\n",
        "        #     pose3d,\n",
        "        #     angle=i // 2,\n",
        "        #     save_path=f\"{HOME}/{OUTPUT_DIR}/{file_name}\",\n",
        "        #     foot_1_trace=foot_1_trace,\n",
        "        #     foot_2_trace=foot_2_trace\n",
        "        # )\n",
        "        draw_3d1(\n",
        "            pose3d,\n",
        "            angle=i // 2,\n",
        "            joint_angle=elbow_angle_3d,\n",
        "            save_path=f\"{HOME}/{OUTPUT_DIR}/{file_name}\",\n",
        "            angle_point=f\"Right Shoulder\"\n",
        "        )\n",
        "\n",
        "    with open(f\"{HOME}/{OUTPUT_DIR}/coordinates.json\", 'w') as json_file:\n",
        "        json.dump(json_data, json_file, indent=4)\n",
        "\n",
        "    with open(f\"{HOME}/{OUTPUT_DIR}/angles.json\", 'w') as angle_file:\n",
        "        json.dump(angle_data, angle_file, indent=4)\n",
        "\n",
        "    excel_file_path = f\"{HOME}/{OUTPUT_DIR}/coordinates.xlsx\"\n",
        "    save_to_excel(json_data, excel_file_path, angle_data)\n",
        "\n",
        "    excel_file_path = f\"{HOME}/pose_distances.xlsx\"\n",
        "    distance_df.to_excel(excel_file_path, index=False)\n",
        "    print(f\"Excel file saved to {excel_file_path}\")\n",
        "\n",
        "    angle_df = pd.DataFrame.from_dict(angle_data, orient='index')\n",
        "    angle_excel_file_path = f\"{HOME}/{OUTPUT_DIR}/angles.xlsx\"\n",
        "    angle_df.to_excel(angle_excel_file_path, index=True)\n",
        "    print(f\"Angle data saved to {angle_excel_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wgUnufBcoxo"
      },
      "outputs": [],
      "source": [
        "# generate_frames(0, len(extracted_data_a))\n",
        "# generate_frames(0, len(extracted_data_b))\n",
        "generate_frames(0, len(extracted_data_c))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWJAh2t9crNI"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "def create_video_from_images(image_folder, video_name, fps=30):\n",
        "    images = [img for img in sorted(os.listdir(image_folder)) if img.endswith(\".png\")]\n",
        "\n",
        "    if not images:\n",
        "        print(\"No PNG images found in the specified directory.\")\n",
        "        return\n",
        "\n",
        "    frame = cv2.imread(os.path.join(image_folder, images[0]))\n",
        "\n",
        "    if frame is None:\n",
        "        print(\"Error reading the first image.\")\n",
        "        return\n",
        "\n",
        "    height, width, _ = frame.shape\n",
        "    video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "\n",
        "    for image in images:\n",
        "        img_path = os.path.join(image_folder, image)\n",
        "        img_frame = cv2.imread(img_path)\n",
        "        if img_frame is None:\n",
        "            print(f\"Error reading image: {img_path}\")\n",
        "            continue\n",
        "        video.write(img_frame)\n",
        "\n",
        "    video.release()\n",
        "    print(f\"Video saved as: {video_name}\")\n",
        "\n",
        "# Call the function:\n",
        "create_video_from_images(OUTPUT_DIR, \"output_video.mp4\", fps=30)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
