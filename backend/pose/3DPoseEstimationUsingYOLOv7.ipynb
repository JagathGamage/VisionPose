{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQvmsyijFxMm"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "DiDPLNDdFPvm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Mar 16 13:00:36 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 572.60                 Driver Version: 572.60         CUDA Version: 12.8     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce MX330         WDDM  |   00000000:02:00.0 Off |                  N/A |\n",
            "| N/A   62C    P0            N/A  / 5001W |       0MiB /   2048MiB |      1%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "dqX5bzLbFs-0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch:  2.5 ; cuda:  cu121\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'nvcc' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Y-eoyQbZF_PM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Jagath\\Desktop\\pose\\yolov7\\yolov7\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-v532W2GD_t"
      },
      "source": [
        "# YOLOv7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "bL1BAojZGJXA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Jagath\\Desktop\\pose\\yolov7\\yolov7\n",
            "c:\\Users\\Jagath\\Desktop\\pose\\yolov7\\yolov7\\yolov7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'yolov7'...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib>=3.2.2 in c:\\python312\\lib\\site-packages (from -r requirements.txt (line 4)) (3.10.1)"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Getting requirements to build wheel did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [33 lines of output]\n",
            "      Traceback (most recent call last):\n",
            "        File \"C:\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
            "          main()\n",
            "        File \"C:\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
            "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
            "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "        File \"C:\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 112, in get_requires_for_build_wheel\n",
            "          backend = _build_backend()\n",
            "                    ^^^^^^^^^^^^^^^^\n",
            "        File \"C:\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 77, in _build_backend\n",
            "          obj = import_module(mod_path)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "        File \"C:\\Python312\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
            "          return _bootstrap._gcd_import(name[level:], package, level)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "        File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "        File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "        File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
            "        File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "        File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "        File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "        File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
            "        File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
            "        File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
            "        File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "        File \"C:\\Users\\Jagath\\AppData\\Local\\Temp\\pip-build-env-m_3mmjq5\\overlay\\Lib\\site-packages\\setuptools\\__init__.py\", line 16, in <module>\n",
            "          import setuptools.version\n",
            "        File \"C:\\Users\\Jagath\\AppData\\Local\\Temp\\pip-build-env-m_3mmjq5\\overlay\\Lib\\site-packages\\setuptools\\version.py\", line 1, in <module>\n",
            "          import pkg_resources\n",
            "        File \"C:\\Users\\Jagath\\AppData\\Local\\Temp\\pip-build-env-m_3mmjq5\\overlay\\Lib\\site-packages\\pkg_resources\\__init__.py\", line 2172, in <module>\n",
            "          register_finder(pkgutil.ImpImporter, find_on_path)\n",
            "                          ^^^^^^^^^^^^^^^^^^^\n",
            "      AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "error: subprocess-exited-with-error\n",
            "\n",
            "× Getting requirements to build wheel did not run successfully.\n",
            "│ exit code: 1\n",
            "╰─> See above for output.\n",
            "\n",
            "note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Collecting numpy<1.24.0,>=1.18.5 (from -r requirements.txt (line 5))\n",
            "  Using cached numpy-1.23.5.tar.gz (10.7 MB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'error'\n"
          ]
        }
      ],
      "source": [
        "%cd {HOME}\n",
        "!git clone https://github.com/WongKinYiu/yolov7\n",
        "%cd {HOME}/yolov7\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "6D3zGdYOGLyJ"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(f\"{HOME}/yolov7\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA2aIiWmGPBY"
      },
      "source": [
        "# Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "zLqn9syzGTUI"
      },
      "outputs": [],
      "source": [
        "# %cd {HOME}\n",
        "# !mkdir input\n",
        "# %cd {HOME}/input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "tJUw77irGYun"
      },
      "outputs": [],
      "source": [
        "SOURCE_VIDEO_A_PATH = f\"{HOME}/input/vid1.mp4\"\n",
        "SOURCE_VIDEO_B_PATH = f\"{HOME}/input/vid-mid.mp4\"\n",
        "SOURCE_VIDEO_C_PATH = f\"{HOME}/input/vid-right.mp4\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W5KVlYmGeXh"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "MMa-1mxeGjLX"
      },
      "outputs": [],
      "source": [
        "from typing import Generator\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import cv2\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "def generate_frames(video_file: str) -> Generator[np.ndarray, None, None]:\n",
        "    video = cv2.VideoCapture(video_file)\n",
        "\n",
        "    while video.isOpened():\n",
        "        success, frame = video.read()\n",
        "\n",
        "        if not success:\n",
        "            break\n",
        "\n",
        "        yield frame\n",
        "\n",
        "    video.release()\n",
        "\n",
        "\n",
        "def plot_image(image: np.ndarray, size: int = 12) -> None:\n",
        "    plt.figure(figsize=(size, size))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image[...,::-1])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "-D6170K0Gmiw"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Tuple\n",
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Point:\n",
        "    x: float\n",
        "    y: float\n",
        "\n",
        "    @property\n",
        "    def int_xy_tuple(self) -> Tuple[int, int]:\n",
        "        return int(self.x), int(self.y)\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Rect:\n",
        "    x: float\n",
        "    y: float\n",
        "    width: float\n",
        "    height: float\n",
        "\n",
        "    @property\n",
        "    def top_left(self) -> Point:\n",
        "        return Point(x=self.x, y=self.y)\n",
        "\n",
        "    @property\n",
        "    def bottom_right(self) -> Point:\n",
        "        return Point(x=self.x + self.width, y=self.y + self.height)\n",
        "\n",
        "    @property\n",
        "    def bottom_center(self) -> Point:\n",
        "        return Point(x=self.x + self.width / 2, y=self.y + self.height)\n",
        "\n",
        "@dataclass\n",
        "class Detection:\n",
        "    rect: Rect\n",
        "    class_id: int\n",
        "    confidence: float\n",
        "    tracker_id: Optional[int] = None\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Color:\n",
        "    r: int\n",
        "    g: int\n",
        "    b: int\n",
        "\n",
        "    @property\n",
        "    def bgr_tuple(self) -> Tuple[int, int, int]:\n",
        "        return self.b, self.g, self.r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "cs_m3lPRGqHa"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def draw_rect(image: np.ndarray, rect: Rect, color: Color, thickness: int = 2) -> np.ndarray:\n",
        "    cv2.rectangle(image, rect.top_left.int_xy_tuple, rect.bottom_right.int_xy_tuple, color.bgr_tuple, thickness)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "oBf2E0hIGtjg"
      },
      "outputs": [
        {
          "ename": "StopIteration",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[83], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m frame_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(generate_frames(video_file\u001b[38;5;241m=\u001b[39mSOURCE_VIDEO_A_PATH))\n\u001b[1;32m----> 3\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mframe_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m plot_image(frame, \u001b[38;5;241m16\u001b[39m)\n",
            "\u001b[1;31mStopIteration\u001b[0m: "
          ]
        }
      ],
      "source": [
        "frame_iterator = iter(generate_frames(video_file=SOURCE_VIDEO_A_PATH))\n",
        "\n",
        "frame = next(frame_iterator)\n",
        "plot_image(frame, 16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3llAwMDoGwU0"
      },
      "source": [
        "# Load YOLOv7 Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOeap7XnG1-r"
      },
      "source": [
        "Download Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eD6ALDreG4nR"
      },
      "outputs": [],
      "source": [
        "%cd {HOME}/yolov7\n",
        "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6e.pt --quiet\n",
        "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6-pose.pt --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wq6TPfmBG6fr"
      },
      "outputs": [],
      "source": [
        "DETECTION_MODEL_WEIGHTS_PATH = f\"{HOME}/yolov7/yolov7-e6e.pt\"\n",
        "POSE_MODEL_WEIGHTS_PATH = f\"{HOME}/yolov7/yolov7-w6-pose.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6mBQ8uBG8u8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw_mz5K7HAyd"
      },
      "source": [
        "Load Detection Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9esFjRVbHCrc"
      },
      "outputs": [],
      "source": [
        "from utils.general import check_img_size\n",
        "from models.experimental import attempt_load\n",
        "\n",
        "detection_model = attempt_load(weights=DETECTION_MODEL_WEIGHTS_PATH, map_location=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBfU-Rx2HEsj"
      },
      "source": [
        "Load Pose Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_MxhlrxHGWd"
      },
      "outputs": [],
      "source": [
        "weigths = torch.load(POSE_MODEL_WEIGHTS_PATH, map_location=device)\n",
        "pose_model = weigths[\"model\"]\n",
        "_ = pose_model.float().eval()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    pose_model.half().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lhEOdUiHKLX"
      },
      "source": [
        "# Single Frame Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WAYboJ8HOxE"
      },
      "source": [
        "Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmV-_P6vHN3b"
      },
      "outputs": [],
      "source": [
        "DETECTION_IMAGE_SIZE = 1920\n",
        "POSE_IMAGE_SIZE = 960\n",
        "STRIDE = 64\n",
        "CONFIDENCE_TRESHOLD = 0.25\n",
        "IOU_TRESHOLD = 0.65"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjeFOLhXHTCu"
      },
      "source": [
        "Pre-process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18SGeybSHURA"
      },
      "outputs": [],
      "source": [
        "from utils.datasets import letterbox\n",
        "from torchvision import transforms\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def detection_pre_process_frame(frame: np.ndarray, device: torch.device) -> torch.Tensor:\n",
        "    img = letterbox(frame, DETECTION_IMAGE_SIZE, STRIDE, auto=True)[0]\n",
        "    img = img[:, :, ::-1].transpose(2, 0, 1)\n",
        "    img = np.ascontiguousarray(img)\n",
        "    img = torch.from_numpy(img).to(device).float()\n",
        "    img /= 255.0\n",
        "    if img.ndimension() == 3:\n",
        "        img = img.unsqueeze(0)\n",
        "    return img\n",
        "\n",
        "def pose_pre_process_frame(frame: np.ndarray, device: torch.device) -> torch.Tensor:\n",
        "    image = letterbox(frame, POSE_IMAGE_SIZE, stride=STRIDE, auto=True)[0]\n",
        "    image = transforms.ToTensor()(image)\n",
        "    image = torch.tensor(np.array([image.numpy()]))\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        image = image.half().to(device)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9CK4GajHZi3"
      },
      "source": [
        "Post-process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYweiAE3HYmu"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "from utils.general import non_max_suppression_kpt, non_max_suppression\n",
        "from utils.plots import output_to_keypoint\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def clip_coords(boxes: np.ndarray, img_shape: Tuple[int, int]):\n",
        "    # Clip bounding xyxy bounding boxes to image shape (height, width)\n",
        "    boxes[:, 0] = np.clip(boxes[:, 0], 0, img_shape[1]) # x1\n",
        "    boxes[:, 1] = np.clip(boxes[:, 1], 0, img_shape[0]) # y1\n",
        "    boxes[:, 2] = np.clip(boxes[:, 2], 0, img_shape[1]) # x2\n",
        "    boxes[:, 3] = np.clip(boxes[:, 3], 0, img_shape[0]) # y2\n",
        "\n",
        "\n",
        "def detection_post_process_output(\n",
        "    output: torch.tensor,\n",
        "    confidence_trashold: float,\n",
        "    iou_trashold: float,\n",
        "    image_size: Tuple[int, int],\n",
        "    scaled_image_size: Tuple[int, int]\n",
        ") -> np.ndarray:\n",
        "    output = non_max_suppression(\n",
        "        prediction=output,\n",
        "        conf_thres=confidence_trashold,\n",
        "        iou_thres=iou_trashold\n",
        "    )\n",
        "    coords = output[0].detach().cpu().numpy()\n",
        "\n",
        "    v_gain = scaled_image_size[0] / image_size[0]\n",
        "    h_gain = scaled_image_size[1] / image_size[1]\n",
        "\n",
        "    coords[:, 0] /= h_gain\n",
        "    coords[:, 1] /= v_gain\n",
        "    coords[:, 2] /= h_gain\n",
        "    coords[:, 3] /= v_gain\n",
        "\n",
        "    clip_coords(coords, image_size)\n",
        "    return coords\n",
        "\n",
        "\n",
        "def post_process_pose(pose: np.ndarray, image_size: Tuple, scaled_image_size: Tuple) -> np.ndarray:\n",
        "    height, width = image_size\n",
        "    scaled_height, scaled_width = scaled_image_size\n",
        "    vertical_factor = height / scaled_height\n",
        "    horizontal_factor = width / scaled_width\n",
        "    result = pose.copy()\n",
        "    for i in range(17):\n",
        "        result[i * 3] = horizontal_factor * result[i * 3]\n",
        "        result[i * 3 + 1] = vertical_factor * result[i * 3 + 1]\n",
        "    return result\n",
        "\n",
        "\n",
        "def pose_post_process_output(\n",
        "    output: torch.tensor,\n",
        "    confidence_trashold: float,\n",
        "    iou_trashold: float,\n",
        "    image_size: Tuple[int, int],\n",
        "    scaled_image_size: Tuple[int, int]\n",
        ") -> np.ndarray:\n",
        "    output = non_max_suppression_kpt(\n",
        "        prediction=output,\n",
        "        conf_thres=confidence_trashold,\n",
        "        iou_thres=iou_trashold,\n",
        "        nc=pose_model.yaml['nc'],\n",
        "        nkpt=pose_model.yaml['nkpt'],\n",
        "        kpt_label=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = output_to_keypoint(output)\n",
        "\n",
        "        for idx in range(output.shape[0]):\n",
        "            output[idx, 7:] = post_process_pose(\n",
        "                output[idx, 7:],\n",
        "                image_size=image_size,\n",
        "                scaled_image_size=scaled_image_size\n",
        "            )\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXVzZBQJHiVh"
      },
      "source": [
        "Annotate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exjK86Q3HjUh"
      },
      "outputs": [],
      "source": [
        "from utils.plots import plot_skeleton_kpts\n",
        "\n",
        "\n",
        "def detect_annotate(image: np.ndarray, detections: np.ndarray, color: Color, thickness: int = 2) -> np.ndarray:\n",
        "    annotated_image = image.copy()\n",
        "    for x_min, y_min, x_max, y_max, confidence, class_id in detections:\n",
        "        rect = Rect(\n",
        "            x=float(x_min),\n",
        "            y=float(y_min),\n",
        "            width=float(x_max - x_min),\n",
        "            height=float(y_max - y_min)\n",
        "        )\n",
        "        annotated_image = draw_rect(image=annotated_image, rect=rect, color=color, thickness=thickness)\n",
        "\n",
        "    return annotated_image\n",
        "\n",
        "\n",
        "def pose_annotate(image: np.ndarray, detections: np.ndarray) -> np.ndarray:\n",
        "    annotated_frame = image.copy()\n",
        "\n",
        "    # for idx in range(detections.shape[0]):\n",
        "    #     pose = detections[idx, 7:].T\n",
        "    #     plot_skeleton_kpts(annotated_frame, pose, 3)\n",
        "\n",
        "    # Iterate over each detected person in the detections\n",
        "    for idx in range(detections.shape[0]):\n",
        "        # Extract keypoints for the pose\n",
        "        pose = detections[idx, 7:]  # Extract keypoints\n",
        "        if pose.ndim > 1:           # Flatten to 1D if needed\n",
        "            pose = pose.flatten()\n",
        "\n",
        "        plot_skeleton_kpts(annotated_frame, pose, 3)  # Plot skeleton keypoints\n",
        "\n",
        "        # Draw each keypoint's index near the keypoint on the frame\n",
        "        for i in range(len(pose) // 3):  # Iterate over each keypoint\n",
        "            x = int(pose[i * 3])         # x-coordinate of the keypoint\n",
        "            y = int(pose[i * 3 + 1])     # y-coordinate of the keypoint\n",
        "            conf = pose[i * 3 + 2]       # confidence score\n",
        "\n",
        "            if conf > 0.5:  # Only display if the confidence is above a threshold\n",
        "                cv2.putText(\n",
        "                    annotated_frame,\n",
        "                    str(i),                # Text to display (index of keypoint)\n",
        "                    (x, y),                # Position to display the text\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    0.5,                   # Font scale\n",
        "                    (255, 255, 255),           # Color (White)\n",
        "                    1,                     # Thickness\n",
        "                    cv2.LINE_AA\n",
        "                )\n",
        "\n",
        "    return annotated_frame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hg-DuvWqHm5Y"
      },
      "source": [
        "Single Frame Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RS4ScyA2HpAH"
      },
      "outputs": [],
      "source": [
        "COLOR = Color(r=255, g=255, b=255)\n",
        "\n",
        "frame_iterator = iter(generate_frames(video_file=SOURCE_VIDEO_A_PATH))\n",
        "\n",
        "frame = next(frame_iterator)\n",
        "\n",
        "detection_pre_processed_frame = detection_pre_process_frame(\n",
        "    frame=frame,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "image_size = frame.shape[:2]\n",
        "scaled_image_size = tuple(detection_pre_processed_frame.size())[2:]\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    detection_output = detection_model(detection_pre_processed_frame)[0].detach().cpu()\n",
        "\n",
        "    detection_output = detection_post_process_output(\n",
        "        output=detection_output,\n",
        "        confidence_trashold=CONFIDENCE_TRESHOLD,\n",
        "        iou_trashold=IOU_TRESHOLD,\n",
        "        image_size=image_size,\n",
        "        scaled_image_size=scaled_image_size\n",
        "    )\n",
        "\n",
        "annotated_frame = detect_annotate(image=frame, detections=detection_output, color=COLOR)\n",
        "\n",
        "plot_image(annotated_frame, 16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gywrvwxwHtDJ"
      },
      "source": [
        "Single Frame Pose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LIjmdGkHugT"
      },
      "outputs": [],
      "source": [
        "def process_frame_and_annotate(frame: np.ndarray) -> np.ndarray:\n",
        "    pose_pre_processed_frame = pose_pre_process_frame(frame=frame.copy(), device=device)\n",
        "\n",
        "    image_size = frame.shape[:2]\n",
        "    scaled_image_size = tuple(pose_pre_processed_frame.size())[2:]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pose_output, _ = pose_model(pose_pre_processed_frame)\n",
        "        pose_output = pose_post_process_output(\n",
        "            output=pose_output,\n",
        "            confidence_trashold=CONFIDENCE_TRESHOLD,\n",
        "            iou_trashold=IOU_TRESHOLD,\n",
        "            image_size=image_size,\n",
        "            scaled_image_size=scaled_image_size\n",
        "        )\n",
        "\n",
        "    annotated_frame = pose_annotate(image=frame, detections=pose_output)\n",
        "\n",
        "    return annotated_frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9r8Z9rrHwz7"
      },
      "outputs": [],
      "source": [
        "frame_iterator = iter(generate_frames(video_file=SOURCE_VIDEO_A_PATH))\n",
        "\n",
        "frame = next(frame_iterator)\n",
        "\n",
        "annotated_frame = process_frame_and_annotate(frame=frame)\n",
        "\n",
        "plot_image(annotated_frame, 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfKaYIwNHzNh"
      },
      "outputs": [],
      "source": [
        "frame_iterator = iter(generate_frames(video_file=SOURCE_VIDEO_B_PATH))\n",
        "\n",
        "frame = next(frame_iterator)\n",
        "\n",
        "annotated_frame = process_frame_and_annotate(frame=frame)\n",
        "\n",
        "plot_image(annotated_frame, 16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyweEnEhH2ZT"
      },
      "source": [
        "# Process Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HF742P31H41M"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "import cv2\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "usage example:\n",
        "\n",
        "video_config = VideoConfig(\n",
        "    fps=30,\n",
        "    width=1920,\n",
        "    height=1080)\n",
        "video_writer = get_video_writer(\n",
        "    target_video_path=TARGET_VIDEO_PATH,\n",
        "    video_config=video_config)\n",
        "\n",
        "for frame in frames:\n",
        "    ...\n",
        "    video_writer.write(frame)\n",
        "\n",
        "video_writer.release()\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# stores information about output video file, width and height of the frame must be equal to input video\n",
        "@dataclass(frozen=True)\n",
        "class VideoConfig:\n",
        "    fps: float\n",
        "    width: int\n",
        "    height: int\n",
        "\n",
        "\n",
        "# create cv2.VideoWriter object that we can use to save output video\n",
        "def get_video_writer(target_video_path: str, video_config: VideoConfig) -> cv2.VideoWriter:\n",
        "    video_target_dir = os.path.dirname(os.path.abspath(target_video_path))\n",
        "    os.makedirs(video_target_dir, exist_ok=True)\n",
        "    return cv2.VideoWriter(\n",
        "        target_video_path,\n",
        "        fourcc=cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
        "        fps=video_config.fps,\n",
        "        frameSize=(video_config.width, video_config.height),\n",
        "        isColor=True\n",
        "    )\n",
        "\n",
        "\n",
        "def get_frame_count(path: str) -> int:\n",
        "    cap = cv2.VideoCapture(SOURCE_VIDEO_B_PATH)\n",
        "    return int(cap.get(cv2.CAP_PROP_FRAME_COUNT))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YM31hvPqH7bI"
      },
      "outputs": [],
      "source": [
        "SOURCE_VIDEO_PATH = SOURCE_VIDEO_B_PATH\n",
        "TARGET_VIDEO_PATH = f\"{HOME}/output/pose-estimation-synchronised-sample-b.mp4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqNF5FWtH9lD"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "\n",
        "COLOR = Color(r=255, g=255, b=255)\n",
        "\n",
        "# initiate video writer\n",
        "video_config = VideoConfig(\n",
        "    fps=25,\n",
        "    width=1920,\n",
        "    height=1080)\n",
        "video_writer = get_video_writer(\n",
        "    target_video_path=TARGET_VIDEO_PATH,\n",
        "    video_config=video_config)\n",
        "\n",
        "# get fresh video frame generator\n",
        "frame_iterator = iter(generate_frames(video_file=SOURCE_VIDEO_PATH))\n",
        "\n",
        "total = get_frame_count(SOURCE_VIDEO_PATH)\n",
        "\n",
        "for i,frame in enumerate(tqdm(frame_iterator, total=total)):\n",
        "    start_time = time.time()\n",
        "    # print(f\"Processing fame {i+1}/{total}...\")\n",
        "\n",
        "    annotated_frame = frame.copy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_size = frame.shape[:2]\n",
        "\n",
        "        #detection\n",
        "        detection_pre_processed_frame = detection_pre_process_frame(\n",
        "            frame=frame,\n",
        "            device=device)\n",
        "        detection_scaled_image_size = tuple(detection_pre_processed_frame.size())[2:]\n",
        "\n",
        "        detection_output = detection_model(detection_pre_processed_frame)[0].detach().cpu()\n",
        "        detection_output = detection_post_process_output(\n",
        "            output=detection_output,\n",
        "            confidence_trashold=CONFIDENCE_TRESHOLD,\n",
        "            iou_trashold=IOU_TRESHOLD,\n",
        "            image_size=image_size,\n",
        "            scaled_image_size=detection_scaled_image_size\n",
        "        )\n",
        "        annotated_frame = detect_annotate(\n",
        "            image=annotated_frame, detections=detection_output, color=COLOR)\n",
        "\n",
        "        # pose\n",
        "        pose_pre_processed_frame = pose_pre_process_frame(\n",
        "            frame=frame,\n",
        "            device=device)\n",
        "        pose_scaled_image_size = tuple(pose_pre_processed_frame.size())[2:]\n",
        "\n",
        "        pose_output = pose_model(pose_pre_processed_frame)[0].detach().cpu()\n",
        "        pose_output = pose_post_process_output(\n",
        "            output=pose_output,\n",
        "            confidence_trashold=CONFIDENCE_TRESHOLD,\n",
        "            iou_trashold=IOU_TRESHOLD,\n",
        "            image_size=image_size,\n",
        "            scaled_image_size=pose_scaled_image_size\n",
        "        )\n",
        "        annotated_frame = pose_annotate(\n",
        "            image=annotated_frame, detections=pose_output)\n",
        "\n",
        "\n",
        "        # Log annotated frame dimensions\n",
        "        # print(f\"Annotated frame dimensions before resize: {annotated_frame.shape}\")\n",
        "\n",
        "        # Resize the annotated frame if dimensions do not match (again)\n",
        "        if annotated_frame.shape[:2] != (1080, 1920):\n",
        "            # print(f\"Resizing annotated frame from {annotated_frame.shape[:2]} to (1080, 1920)...\")\n",
        "            annotated_frame = cv2.resize(annotated_frame, (1920, 1080))\n",
        "\n",
        "        # Check if the annotated frame's dimensions are valid after resize\n",
        "        # print(f\"Annotated frame dimensions after resize: {annotated_frame.shape}\")\n",
        "\n",
        "        # Check if frame is valid (not None) and has the right dimensions\n",
        "        if annotated_frame is not None and annotated_frame.shape[:2] == (1080, 1920):\n",
        "            video_writer.write(annotated_frame)\n",
        "        else:\n",
        "            print(f\"Frame {i+1} is invalid or has the wrong dimensions!\")\n",
        "\n",
        "        # save video frame\n",
        "        # video_writer.write(annotated_frame)\n",
        "\n",
        "    # Print how long the processing for this frame took\n",
        "    # print(f\"Frame {i+1} processed in {time.time() - start_time:.4f} seconds\")\n",
        "\n",
        "# close output video\n",
        "video_writer.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkVHMbSWKSOa"
      },
      "source": [
        "# Process and Save Results to Join"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ow4-v7_bKXtL"
      },
      "outputs": [],
      "source": [
        "from typing import Union\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "def create_parent_dir(file_path: str) -> None:\n",
        "    file_directory = os.path.dirname(os.path.abspath(file_path))\n",
        "    os.makedirs(file_directory, exist_ok=True)\n",
        "\n",
        "\n",
        "def dump_json_file(file_path: str, content: Union[list, dict], **kwargs) -> None:\n",
        "    create_parent_dir(file_path=file_path)\n",
        "    with open(file_path, \"w\") as file:\n",
        "        json.dump(content, file, **kwargs)\n",
        "\n",
        "def dump_excel_file(file_path: str, content: list) -> None:\n",
        "    create_parent_dir(file_path=file_path)\n",
        "\n",
        "    # Convert list of dictionaries into a pandas DataFrame\n",
        "    df = pd.DataFrame(content)\n",
        "\n",
        "    # Dump data into an Excel file\n",
        "    df.to_excel(file_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOOjTs_49ewU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "# Define colors\n",
        "KEYPOINT_COLOR = (255, 0, 0)  # Blue for keypoints\n",
        "CONNECTION_COLOR = (0, 255, 0)  # Green for connections\n",
        "ANGLE_COLOR_SMALL = (0, 0, 255)  # Red for small angles\n",
        "ANGLE_COLOR_LARGE = (0, 255, 0)  # Green for larger angles\n",
        "\n",
        "\n",
        "CONNECTIONS = [(8, 6), (6, 12)]\n",
        "\n",
        "\n",
        "def angle(kpts,p1,p2,p3):\n",
        "    coord = []\n",
        "    no_kpt = len(kpts)//3\n",
        "    for i in range(no_kpt):\n",
        "      cx,cy = kpts[3*i], kpts[3*i +1]\n",
        "      conf = kpts[3*i +2]\n",
        "      coord.append([i,cx,cy,conf])\n",
        "\n",
        "    points = (p1,p2,p3)\n",
        "\n",
        "\n",
        "    x1,y1 = coord[p1][1:3]\n",
        "    x2,y2 = coord[p2][1:3]\n",
        "    x3,y3 = coord[p3][1:3]\n",
        "\n",
        "    angle = math.degrees(math.atan2(y3-y2,x3-x2)-math.atan2(y1-y2,x1-x2))\n",
        "\n",
        "    if angle < 0:\n",
        "      angle += 360\n",
        "\n",
        "    if angle > 180:\n",
        "      angle = 360 - angle\n",
        "\n",
        "    return int(angle)\n",
        "\n",
        "\n",
        "def draw_angle_on_frame(frame, angle, position):\n",
        "    # font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    # cv2.putText(frame, f\"Right Knee Angle: {angle} degrees\", position, font, 0.6, (0, 255, 0), 2)\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 1\n",
        "    font_thickness = 2\n",
        "    text_color = (0, 255, 0)  # Green text\n",
        "    border_color = (0, 0, 255)  # Blue border\n",
        "    border_thickness = 2\n",
        "    padding = 5  # Padding around the text inside the border\n",
        "\n",
        "    # Create the text string\n",
        "    text = f\"Right Shoulder Angle: {angle} degrees\"\n",
        "\n",
        "    # Get the size of the text\n",
        "    text_size = cv2.getTextSize(text, font, font_scale, font_thickness)\n",
        "    text_width, text_height = text_size[0]\n",
        "    baseline = text_size[1]\n",
        "\n",
        "    # Define the top-left and bottom-right corners of the border\n",
        "    top_left = (position[0] - padding, position[1] - text_height - baseline - padding)\n",
        "    bottom_right = (position[0] + text_width + padding, position[1] + baseline)\n",
        "\n",
        "    # Draw the rectangle border\n",
        "    cv2.rectangle(frame, top_left, bottom_right, border_color, border_thickness)\n",
        "\n",
        "    # Position of the text inside the border\n",
        "    text_position = (position[0], position[1] - baseline)\n",
        "\n",
        "    # Draw the text\n",
        "    cv2.putText(frame, text, text_position, font, font_scale, text_color, font_thickness)\n",
        "\n",
        "\n",
        "# function for detecting foot contacts\n",
        "def detect_foot_contact(pose_output, prev_y_positions, prev_contact_times, fps, frame_number):\n",
        "    \"\"\"\n",
        "    Detects the timestamps when the foot makes contact with the ground and calculates step duration.\n",
        "\n",
        "    Args:\n",
        "        pose_output (np.ndarray): Pose keypoints output from YOLOv7.\n",
        "        prev_y_positions (dict): Stores previous y-coordinates of ankles.\n",
        "        prev_contact_times (dict): Stores previous timestamps for each foot.\n",
        "        fps (float): Frames per second of the video.\n",
        "        frame_number (int): Current frame index.\n",
        "\n",
        "    Returns:\n",
        "        contact_events (list): List of (foot, timestamp, step_duration) tuples.\n",
        "        prev_y_positions (dict): Updated y-coordinate values of ankles.\n",
        "        prev_contact_times (dict): Updated last contact times.\n",
        "    \"\"\"\n",
        "    contact_events = []\n",
        "\n",
        "    if pose_output.shape[0] == 0:\n",
        "        return contact_events, prev_y_positions, prev_contact_times\n",
        "\n",
        "    for idx in range(pose_output.shape[0]):\n",
        "        keypoints = pose_output[idx, 7:]\n",
        "\n",
        "        left_ankle_y = keypoints[15 * 3 + 1]  # y-coordinate of the left ankle\n",
        "        right_ankle_y = keypoints[16 * 3 + 1] # y-coordinate of the right ankle\n",
        "\n",
        "        # Check for contact: if y-coordinate stabilize/has no downward movements\n",
        "        for foot, ankle_y in [(\"left\", left_ankle_y), (\"right\", right_ankle_y)]:\n",
        "            if foot in prev_y_positions:\n",
        "                prev_y = prev_y_positions[foot]\n",
        "                velocity = ankle_y - prev_y   # Change in y (negative values means moving down)\n",
        "                print(velocity)\n",
        "\n",
        "                if abs(velocity) < 2:\n",
        "                    timestamp = frame_number / fps\n",
        "                    step_duration = timestamp - prev_contact_times[foot] if prev_contact_times[foot] is not None else None\n",
        "\n",
        "                    contact_events.append((foot, timestamp, step_duration))\n",
        "                    print(f\"{foot.capitalize()} foot contact at {timestamp:.2f} sec, Step duration: {step_duration:.2f} sec\" if step_duration else f\"{foot.capitalize()} foot contact at {timestamp:.2f} sec\")\n",
        "\n",
        "                    prev_contact_times[foot] = timestamp\n",
        "\n",
        "            prev_y_positions[foot] = ankle_y\n",
        "\n",
        "    return contact_events, prev_y_positions, prev_contact_times\n",
        "\n",
        "\n",
        "def process_and_dump(source_video_path: str, target_json_path: str, target_excel_path:str, target_video_path: str, angle_plot_path: str, task_output_path: str) -> None:\n",
        "    frame_iterator = iter(generate_frames(video_file=source_video_path))\n",
        "    total = get_frame_count(source_video_path)\n",
        "    entries = []\n",
        "    angle_changes = []\n",
        "    shoulder_angle = []\n",
        "    contact_events = []\n",
        "\n",
        "    pre_y_positions = {\"left\": None, \"right\": None}\n",
        "    pre_contact_times = {\"left\": None, \"right\": None}\n",
        "\n",
        "    # Keypoint indices for specific landmarks\n",
        "    SHOULDER_INDEX = 8\n",
        "    ELBOW_INDEX = 6\n",
        "    WRIST_INDEX = 12\n",
        "\n",
        "    # Open video for writing\n",
        "    cap = cv2.VideoCapture(source_video_path)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frame_width = int(cap.get(3))\n",
        "    frame_height = int(cap.get(4))\n",
        "    out = cv2.VideoWriter(target_video_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    frame_number = 0\n",
        "\n",
        "    for frame in tqdm(frame_iterator, total=total):\n",
        "        image_size = frame.shape[:2]\n",
        "\n",
        "        #detection\n",
        "        detection_pre_processed_frame = detection_pre_process_frame(frame=frame, device=device)\n",
        "        detection_scaled_image_size = tuple(detection_pre_processed_frame.size())[2:]\n",
        "\n",
        "        detection_output = detection_model(detection_pre_processed_frame)[0].detach().cpu()\n",
        "        detection_output = detection_post_process_output(\n",
        "            output=detection_output,\n",
        "            confidence_trashold=CONFIDENCE_TRESHOLD,\n",
        "            iou_trashold=IOU_TRESHOLD,\n",
        "            image_size=image_size,\n",
        "            scaled_image_size=detection_scaled_image_size\n",
        "        )\n",
        "\n",
        "        # pose\n",
        "        pose_pre_processed_frame = pose_pre_process_frame(frame=frame, device=device)\n",
        "        pose_scaled_image_size = tuple(pose_pre_processed_frame.size())[2:]\n",
        "\n",
        "        pose_output = pose_model(pose_pre_processed_frame)[0].detach().cpu()\n",
        "        pose_output1 = pose_model(pose_pre_processed_frame)[0].detach().cpu()\n",
        "        pose_output = pose_post_process_output(\n",
        "            output=pose_output,\n",
        "            confidence_trashold=CONFIDENCE_TRESHOLD,\n",
        "            iou_trashold=IOU_TRESHOLD,\n",
        "            image_size=image_size,\n",
        "            scaled_image_size=pose_scaled_image_size\n",
        "        )\n",
        "\n",
        "        pose_output1 = non_max_suppression_kpt(\n",
        "            pose_output1,\n",
        "            0.25,\n",
        "            0.65,\n",
        "            nc=pose_model.yaml['nc'],\n",
        "            nkpt=pose_model.yaml['nkpt'],\n",
        "            kpt_label=True\n",
        "        )\n",
        "        pose_output1 = output_to_keypoint(pose_output1)\n",
        "\n",
        "        for idx in range(pose_output1.shape[0]):\n",
        "          kpts=pose_output1[idx,7:].T\n",
        "          elbow_angle=angle(kpts, 8, 6, 12)\n",
        "\n",
        "          # Draw the angle on the frame\n",
        "          draw_angle_on_frame(frame, elbow_angle, (50, 50))\n",
        "\n",
        "          # Draw keypoints, connections, and the angle arc at the elbow\n",
        "          # draw_keypoints_and_connections(frame, kpts)\n",
        "          elbow_position = (int(kpts[3 * ELBOW_INDEX]), int(kpts[3 * ELBOW_INDEX + 1]))\n",
        "          # draw_angle_arc(frame, elbow_angle, elbow_position)\n",
        "\n",
        "          # print(f\"Elbow Angle (Frame {frame_number}): {elbow_angle:.2f} degrees\")\n",
        "\n",
        "          # Save angle and corresponding time for plotting\n",
        "          angle_changes.append((frame_number / fps, elbow_angle))\n",
        "          shoulder_angle.append((frame_number, elbow_angle))\n",
        "\n",
        "        # Save frame to video\n",
        "        out.write(frame)\n",
        "\n",
        "        entry = {\n",
        "            \"detection\": detection_output.tolist(),\n",
        "            \"pose\": pose_output.tolist()\n",
        "        }\n",
        "        entries.append(entry)\n",
        "\n",
        "        # Detect foot contact events and step durations\n",
        "        frame_contact_times, prev_y_positions, prev_contact_times = detect_foot_contact(\n",
        "            pose_output, prev_y_positions, prev_contact_times, fps, framae_number\n",
        "        )\n",
        "        contact_events.extends(frame_contact_times)\n",
        "\n",
        "        frame_number += 1\n",
        "\n",
        "    # Release the video writer and video capture\n",
        "    out.release()\n",
        "    cap.release()\n",
        "\n",
        "    # Save to JSON\n",
        "    dump_json_file(file_path=target_json_path, content=entries)\n",
        "\n",
        "    # Save to Excel\n",
        "    # df = pd.DataFrame(entries)\n",
        "    # create_parent_dir(file_path=target_excel_path)\n",
        "    # df.to_excel(target_excel_path, index=False)\n",
        "    dump_excel_file(file_path=target_excel_path, content=entries)\n",
        "\n",
        "    # Save frame angles to Excel\n",
        "    angle_df = pd.DataFrame(shoulder_angle, columns=[\"Frame\", \"Right Shoulder Angle (degrees)\"])\n",
        "    angle_df.to_excel(target_excel_path, index=False)\n",
        "\n",
        "    # Plotting the angle changes over time\n",
        "    times, angles = zip(*angle_changes)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(times, angles, label=\"Right Shoulder Angle\", color='b')\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Right Shoulder Angle Changes Over Time\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    # plt.show()\n",
        "\n",
        "    # Save the plot to the specified path\n",
        "    plt.savefig(angle_plot_path)\n",
        "    plt.close()\n",
        "\n",
        "    # Step durations\n",
        "    df = pd.DataFrame(contact_events, columns=[\"Foot\", \"Timestamp (s)\", \"Step Duration\"])\n",
        "    df.to_excel(f\"{target_excel_path}/steps_duration.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aelc_vC-Hiz"
      },
      "outputs": [],
      "source": [
        "process_and_dump(SOURCE_VIDEO_A_PATH, f\"{HOME}/output/pose-estimation-synchronised-sample-a.json\", f\"{HOME}/output/right-shoulder-angle-a.xlsx\", f\"{HOME}/output/right-shoulder-angles-sample-a.mp4\", f\"{HOME}/output/right-shoulder-angles-sample-a.png\", f\"{HOME}/output\")\n",
        "process_and_dump(SOURCE_VIDEO_B_PATH, f\"{HOME}/output/pose-estimation-synchronised-sample-b.json\", f\"{HOME}/output/right-shoulder-angle-b.xlsx\", f\"{HOME}/output/right-shoulder-angles-sample-b.mp4\", f\"{HOME}/output/right-shoulder-angles-sample-b.png\", f\"{HOME}/output\")\n",
        "process_and_dump(SOURCE_VIDEO_C_PATH, f\"{HOME}/output/pose-estimation-synchronised-sample-c.json\", f\"{HOME}/output/right-shoulder-angle-c.xlsx\", f\"{HOME}/output/right-shoulder-angles-sample-c.mp4\", f\"{HOME}/output/right-shoulder-angles-sample-c.png\", f\"{HOME}/output\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mhbnGsVio21"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('..')\n",
        "%cd content\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhUpYQlxdWUj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "excel_file_a = f\"{HOME}/output/right-shoulder-angle-a.xlsx\"\n",
        "excel_file_b = f\"{HOME}/output/right-shoulder-angle-b.xlsx\"\n",
        "excel_file_c = f\"{HOME}/output/right-shoulder-angle-c.xlsx\"\n",
        "\n",
        "file_paths = [excel_file_a, excel_file_b, excel_file_c]\n",
        "colors = ['blue', 'green', 'red']  # Colors for each plot\n",
        "labels = ['Left Camera', 'Middle Camera', 'Right Camera']  # Labels for the legend\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for file_path, color, label in zip(file_paths, colors, labels):\n",
        "    # Read the Excel file\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "    # Plot Frame vs. Angle\n",
        "    plt.plot(df['Frame'], df['Right Shoulder Angle (degrees)'], color=color, label=label)\n",
        "\n",
        "# Label the axes and add a title\n",
        "plt.xlabel('Frame')\n",
        "plt.ylabel('Right Shoulder Angle (degrees)')\n",
        "plt.title('Right Shoulder Angle Changes Over Time')\n",
        "\n",
        "# Add a legend to distinguish between the files\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "# plt.show()\n",
        "\n",
        "# Save the plot as a PNG file\n",
        "plt.savefig(f'{HOME}/output/right_shoulder_angle_changes_over_time.png', format='png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSzbp6HO91QS"
      },
      "source": [
        "# **Animation Creation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq7y1rb-9-MH"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXmjxA_Q-m3_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('..')\n",
        "%cd content\n",
        "# print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zt6Tp79A-Ew6"
      },
      "outputs": [],
      "source": [
        "FRAME_WIDTH = 1920\n",
        "FRAME_HEIGHT = 1080"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvVwauiy-LRF"
      },
      "outputs": [],
      "source": [
        "\n",
        "VIEW_X_MIN = - 500\n",
        "VIEW_X_MAX = 500\n",
        "VIEW_Y_MIN = - 500\n",
        "VIEW_Y_MAX = 500\n",
        "VIEW_Z_MIN = 0\n",
        "VIEW_Z_MAX = 1000\n",
        "\n",
        "POSE_ANCHORS = [\n",
        "    [0,1],\n",
        "    [0,2],\n",
        "    [1,3],\n",
        "    [2,4],\n",
        "    [5,6],\n",
        "    [5,7],\n",
        "    [6,8],\n",
        "    [7,9],\n",
        "    [8,10],\n",
        "    [5,11],\n",
        "    [6,12],\n",
        "    [11,12],\n",
        "    [11,13],\n",
        "    [12,14],\n",
        "    [13,15],\n",
        "    [14,16]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPH87PnU-wXE"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import json\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class FrameData:\n",
        "    pose: np.ndarray\n",
        "    detection: np.ndarray\n",
        "\n",
        "\n",
        "def load_json(path: str) -> dict:\n",
        "    with open(path) as f:\n",
        "        contents = f.read()\n",
        "        return json.loads(contents)\n",
        "\n",
        "\n",
        "def load_extracted_data(path: str) -> List[FrameData]:\n",
        "    raw = load_json(path)\n",
        "    return [\n",
        "        FrameData(\n",
        "            pose=entry['pose'],\n",
        "            detection=entry['detection']\n",
        "        )\n",
        "        for entry\n",
        "        in raw\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3H4i975_o4U"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY6clvhK-tNV"
      },
      "outputs": [],
      "source": [
        "EXTRACTED_DATA_A_PATH = f\"{HOME}/output/pose-estimation-synchronised-sample-a.json\"\n",
        "EXTRACTED_DATA_B_PATH = f\"{HOME}/output/pose-estimation-synchronised-sample-b.json\"\n",
        "EXTRACTED_DATA_C_PATH = f\"{HOME}/output/pose-estimation-synchronised-sample-c.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3rbQYA6-0Iw"
      },
      "outputs": [],
      "source": [
        "extracted_data_a = load_extracted_data(EXTRACTED_DATA_A_PATH)\n",
        "extracted_data_b = load_extracted_data(EXTRACTED_DATA_B_PATH)\n",
        "extracted_data_c = load_extracted_data(EXTRACTED_DATA_C_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33ZkyzzC-2vi"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, List, Tuple\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Pose:\n",
        "    x: np.ndarray\n",
        "    y: np.ndarray\n",
        "    confidence: np.ndarray\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, data: List[float]) -> Pose:\n",
        "        x, y, confidence = [], [], []\n",
        "        for i in range(17):\n",
        "            x.append(data[7 + i * 3])\n",
        "            y.append(data[7 + i * 3 + 1])\n",
        "            confidence.append(data[7 + i * 3 + 2])\n",
        "        return Pose(\n",
        "            x=np.array(x),\n",
        "            y=np.array(y),\n",
        "            confidence=np.array(confidence)\n",
        "        )\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Pose3D:\n",
        "    x: np.ndarray\n",
        "    y: np.ndarray\n",
        "    z: np.ndarray\n",
        "\n",
        "    @classmethod\n",
        "    def from2D(cls, pose_a: Pose, pose_b: Pose, pose_c: Pose) -> Pose3D:\n",
        "        return Pose3D(\n",
        "            x=pose_a.x,\n",
        "            y=pose_b.x,\n",
        "            z=(pose_a.y + pose_b.y + pose_c.y) / 3\n",
        "        )\n",
        "\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional\n",
        "\n",
        "# Point class represents a point with x and y coordinates\n",
        "@dataclass\n",
        "class Point:\n",
        "    x: float\n",
        "    y: float\n",
        "\n",
        "# Detection class represents a bounding box around a detected object\n",
        "@dataclass\n",
        "class Detection:\n",
        "    x_min: float\n",
        "    y_min: float\n",
        "    x_max: float\n",
        "    y_max: float\n",
        "    confidence: float\n",
        "    class_id: int\n",
        "\n",
        "    @property\n",
        "    def width(self) -> float:\n",
        "        return self.x_max - self.x_min\n",
        "\n",
        "    @property\n",
        "    def height(self) -> float:\n",
        "        return self.y_max - self.y_min\n",
        "\n",
        "    @property\n",
        "    def center(self) -> Point:\n",
        "        return Point(\n",
        "            x=(self.x_min + self.x_max) / 2,\n",
        "            y=(self.y_min + self.y_max) / 2\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, data: List[float]) -> 'Detection':\n",
        "        return Detection(\n",
        "            x_min=float(data[0]),\n",
        "            y_min=float(data[1]),\n",
        "            x_max=float(data[2]),\n",
        "            y_max=float(data[3]),\n",
        "            confidence=float(data[4]),\n",
        "            class_id=int(data[5])\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def filter(cls, detections: List['Detection'], person_class_id: int = 0) -> Optional['Detection']:\n",
        "        # Filter detections for the person class (default class ID is 0)\n",
        "        filtered_detections = [\n",
        "            detection for detection in detections\n",
        "            if detection.class_id == person_class_id\n",
        "        ]\n",
        "        # Return the first detection of a person if one exists\n",
        "        return filtered_detections[0] if filtered_detections else None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuZs5YAn_Hf0"
      },
      "outputs": [],
      "source": [
        "detections = [Detection.load(detection) for detection in extracted_data_a[0].detection]\n",
        "detection_person = Detection.filter(detections)\n",
        "\n",
        "# Check if a person was detected\n",
        "if detection_person is not None:\n",
        "    pose_a = Pose.load(extracted_data_a[0].pose[0])  # Load the person's pose data\n",
        "else:\n",
        "    pose_a = None  # Handle the case where no person is detected\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehx8w7SJ_J2m"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check if pose_a is valid (i.e., person detected)\n",
        "if pose_a is not None:\n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "    plt.scatter(pose_a.x, pose_a.y, color=\"red\")\n",
        "    plt.xlim([0, FRAME_WIDTH])\n",
        "    plt.ylim([0, FRAME_HEIGHT])\n",
        "    plt.gca().invert_yaxis()  # Invert y-axis if needed to match the video coordinates\n",
        "    plt.show()\n",
        "else:\n",
        "    # Show a completely blank page\n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "    plt.xlim([0, FRAME_WIDTH])\n",
        "    plt.ylim([0, FRAME_HEIGHT])\n",
        "    plt.gca().invert_yaxis()  # Match the y-axis orientation with the video\n",
        "    plt.show()  # Show blank plot without text or scatter points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxOKcY51_MUm"
      },
      "outputs": [],
      "source": [
        "if detection_person is not None:\n",
        "    pose_b = Pose.load(extracted_data_b[0].pose[0])  # Load the person's pose data\n",
        "else:\n",
        "    pose_b = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXN6l5zo_boI"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check if pose_a is valid (i.e., person detected)\n",
        "if pose_b is not None:\n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "    plt.scatter(pose_b.x, pose_b.y, color=\"red\")\n",
        "    plt.xlim([0, FRAME_WIDTH])\n",
        "    plt.ylim([0, FRAME_HEIGHT])\n",
        "    plt.gca().invert_yaxis()  # Invert y-axis if needed to match the video coordinates\n",
        "    plt.show()\n",
        "else:\n",
        "    # Show a completely blank page\n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "    plt.xlim([0, FRAME_WIDTH])\n",
        "    plt.ylim([0, FRAME_HEIGHT])\n",
        "    plt.gca().invert_yaxis()  # Match the y-axis orientation with the video\n",
        "    plt.show()  # Show blank plot without text or scatter points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFvEuBcr9zOj"
      },
      "outputs": [],
      "source": [
        "if detection_person is not None:\n",
        "    pose_c = Pose.load(extracted_data_c[0].pose[0])  # Load the person's pose data\n",
        "else:\n",
        "    pose_c = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGQMFPL795Z2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check if pose_a is valid (i.e., person detected)\n",
        "if pose_c is not None:\n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "    plt.scatter(pose_c.x, pose_c.y, color=\"red\")\n",
        "    plt.xlim([0, FRAME_WIDTH])\n",
        "    plt.ylim([0, FRAME_HEIGHT])\n",
        "    plt.gca().invert_yaxis()  # Invert y-axis if needed to match the video coordinates\n",
        "    plt.show()\n",
        "else:\n",
        "    # Show a completely blank page\n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "    plt.xlim([0, FRAME_WIDTH])\n",
        "    plt.ylim([0, FRAME_HEIGHT])\n",
        "    plt.gca().invert_yaxis()  # Match the y-axis orientation with the video\n",
        "    plt.show()  # Show blank plot without text or scatter points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhpIBk_N_eHC"
      },
      "outputs": [],
      "source": [
        "detections = [Detection.load(detection) for detection in extracted_data_a[0].detection]\n",
        "detection_a = Detection.filter(detections, 0)\n",
        "pose_a = Pose.load(extracted_data_a[0].pose[0])\n",
        "BASELINE_HEIGHT_A = detection_a.height\n",
        "BASELINE_VERTICAL_OFFSET_A = detection_a.y_max - pose_a.y.max()\n",
        "print(\"BASELINE_HEIGHT_A\", BASELINE_HEIGHT_A)\n",
        "print(\"BASELINE_VERTICAL_OFFSET_A\", BASELINE_VERTICAL_OFFSET_A)\n",
        "\n",
        "detections = [Detection.load(detection) for detection in extracted_data_b[0].detection]\n",
        "detection_b = Detection.filter(detections, 0)\n",
        "pose_b = Pose.load(extracted_data_b[0].pose[0])\n",
        "BASELINE_VERTICAL_OFFSET_B = detection_b.y_max - pose_b.y.max()\n",
        "BASELINE_HEIGHT_B = detection_b.height\n",
        "print(\"BASELINE_HEIGHT_B\", BASELINE_HEIGHT_B)\n",
        "print(\"BASELINE_VERTICAL_OFFSET_B\", BASELINE_VERTICAL_OFFSET_B)\n",
        "\n",
        "detections = [Detection.load(detection) for detection in extracted_data_c[0].detection]\n",
        "detection_c = Detection.filter(detections, 0)\n",
        "pose_c = Pose.load(extracted_data_c[0].pose[0])\n",
        "BASELINE_VERTICAL_OFFSET_C = detection_c.y_max - pose_c.y.max()\n",
        "BASELINE_HEIGHT_C = detection_c.height\n",
        "print(\"BASELINE_HEIGHT_C\", BASELINE_HEIGHT_C)\n",
        "print(\"BASELINE_VERTICAL_OFFSET_C\", BASELINE_VERTICAL_OFFSET_C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLlRDtPS_g7d"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Tuple\n",
        "\n",
        "\n",
        "def calibrate(\n",
        "    data: FrameData,\n",
        "    frame_height: int,\n",
        "    baseline_pose_height: float,\n",
        "    baseline_vertical_offset: float\n",
        ") -> Optional[Tuple[Pose, Point]]:\n",
        "    detections = [Detection.load(detection) for detection in data.detection]\n",
        "    detection_person = Detection.filter(detections, 0)\n",
        "    # detection_ball = Detection.filter(detections, 32)\n",
        "\n",
        "    if detection_person is None:\n",
        "        return None\n",
        "    # if detection_ball is None:\n",
        "    #     return None\n",
        "    if len(data.pose) != 1:\n",
        "        return None\n",
        "\n",
        "    # ball_x, ball_y = detection_ball.center.int_xy_tuple\n",
        "\n",
        "    pose = Pose.load(data.pose[0])\n",
        "    pose.y = frame_height - pose.y\n",
        "    # ball_y = frame_height - ball_y\n",
        "\n",
        "    x_shift = (pose.x.max() + pose.x.min()) / 2\n",
        "    y_shift = pose.y.min() - baseline_vertical_offset\n",
        "\n",
        "    pose.x = pose.x - x_shift\n",
        "    # ball_x = ball_x - x_shift\n",
        "    pose.y = (pose.y - y_shift) * 1000 / baseline_pose_height\n",
        "    # ball_y = (ball_y - y_shift) * 1000 / baseline_pose_height\n",
        "    return pose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7mg0iDD_kXe"
      },
      "outputs": [],
      "source": [
        "pose_a = calibrate(extracted_data_a[0], FRAME_HEIGHT, BASELINE_HEIGHT_A, BASELINE_VERTICAL_OFFSET_A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gCmim3Z_mX1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(12, 12))\n",
        "plt.scatter(pose_a.x, pose_a.y, color=\"red\")\n",
        "# plt.scatter([point_a.x], [point_a.y], color=\"blue\")\n",
        "plt.xlim([VIEW_X_MIN, VIEW_X_MAX])\n",
        "plt.ylim([VIEW_Z_MIN, VIEW_Z_MAX])\n",
        "ax = plt.gca()\n",
        "ax.set_aspect('equal', adjustable='box')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34TOY95T_oWv"
      },
      "outputs": [],
      "source": [
        "pose_b = calibrate(extracted_data_b[0], FRAME_HEIGHT, BASELINE_HEIGHT_B, BASELINE_VERTICAL_OFFSET_B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9w6f-GO_qgy"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(12, 12))\n",
        "plt.scatter(pose_b.x, pose_b.y, color=\"red\")\n",
        "# plt.scatter([point_b.x], [point_b.y], color=\"blue\")\n",
        "plt.xlim([VIEW_X_MIN, VIEW_X_MAX])\n",
        "plt.ylim([VIEW_Z_MIN, VIEW_Z_MAX])\n",
        "ax = plt.gca()\n",
        "ax.set_aspect('equal', adjustable='box')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcbmwpb3-JF2"
      },
      "outputs": [],
      "source": [
        "pose_c = calibrate(extracted_data_c[0], FRAME_HEIGHT, BASELINE_HEIGHT_C, BASELINE_VERTICAL_OFFSET_C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUiqBoPy-eqt"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(12, 12))\n",
        "plt.scatter(pose_c.x, pose_c.y, color=\"red\")\n",
        "# plt.scatter([point_b.x], [point_b.y], color=\"blue\")\n",
        "plt.xlim([VIEW_X_MIN, VIEW_X_MAX])\n",
        "plt.ylim([VIEW_Z_MIN, VIEW_Z_MAX])\n",
        "ax = plt.gca()\n",
        "ax.set_aspect('equal', adjustable='box')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKvTBrEA_wPM"
      },
      "outputs": [],
      "source": [
        "pose_a = calibrate(extracted_data_a[0], FRAME_HEIGHT, BASELINE_HEIGHT_A, BASELINE_VERTICAL_OFFSET_A)\n",
        "pose_b = calibrate(extracted_data_b[0], FRAME_HEIGHT, BASELINE_HEIGHT_B, BASELINE_VERTICAL_OFFSET_B)\n",
        "pose_c = calibrate(extracted_data_c[0], FRAME_HEIGHT, BASELINE_HEIGHT_C, BASELINE_VERTICAL_OFFSET_C)\n",
        "\n",
        "pose3d = Pose3D.from2D(pose_a=pose_a, pose_b=pose_b, pose_c=pose_c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFu4TCCO_2sX"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, List\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "\n",
        "class Trace:\n",
        "\n",
        "    def __init__(self, history: int = 10):\n",
        "        self.x = deque(maxlen=history)\n",
        "        self.y = deque(maxlen=history)\n",
        "        self.z = deque(maxlen=history)\n",
        "\n",
        "    def append(self, x: float, y: float, z: float):\n",
        "        self.x.append(x)\n",
        "        self.y.append(y)\n",
        "        self.z.append(z)\n",
        "\n",
        "    def get_state(self) -> Tuple[List[float], List[float], List[float]]:\n",
        "        return list(self.x), list(self.y), list(self.z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hq2UHGhvbcvU"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def calculate_angle_3d(a, b, c):\n",
        "\n",
        "    # Calculate the angle between three points in 3D space.\n",
        "\n",
        "    # Parameters:\n",
        "    # a, b, c: 3D points as numpy arrays where b is the vertex (shoulder),\n",
        "    #          a is one end (elbow), and c is the other end (hip).\n",
        "\n",
        "    # Returns:\n",
        "    # angle: The angle in degrees.\n",
        "\n",
        "    a = np.array(a)  # First point (e.g., elbow)\n",
        "    b = np.array(b)  # Middle point (e.g., shoulder)\n",
        "    c = np.array(c)  # End point (e.g., hip)\n",
        "\n",
        "    # Vector BA and BC\n",
        "    ba = a - b\n",
        "    bc = c - b\n",
        "\n",
        "    # Calculate the cosine of the angle using the dot product formula\n",
        "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
        "\n",
        "    # Clip the cosine value to the range [-1, 1] to avoid numerical errors\n",
        "    cosine_angle = np.clip(cosine_angle, -1.0, 1.0)\n",
        "\n",
        "    # Calculate the angle in radians and then convert to degrees\n",
        "    angle = np.arccos(cosine_angle) * (180.0 / np.pi)\n",
        "\n",
        "    return angle\n",
        "\n",
        "\n",
        "def draw_3d1(\n",
        "    pose3d: Pose3D,\n",
        "    joint_angle: float = 0.0,\n",
        "    angle: int = 0,\n",
        "    save_path: Optional[str] = None,\n",
        "    foot_1_trace: Optional[Trace] = None,\n",
        "    foot_2_trace: Optional[Trace] = None,\n",
        "    angle_point: Optional[str] = \"\",\n",
        ") -> None:\n",
        "\n",
        "    plt.style.use('dark_background')\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 12))\n",
        "    ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "    ax.xaxis.pane.fill = False\n",
        "    ax.yaxis.pane.fill = False\n",
        "    ax.zaxis.pane.fill = False\n",
        "\n",
        "    ax.axes.set_xlim3d(left=VIEW_X_MIN, right=VIEW_X_MAX)\n",
        "    ax.axes.set_ylim3d(bottom=VIEW_Y_MIN, top=VIEW_Y_MAX)\n",
        "    ax.axes.set_zlim3d(bottom=VIEW_Z_MIN, top=VIEW_Z_MAX)\n",
        "\n",
        "    # Plot the pose anchors\n",
        "    for pose_anchor in POSE_ANCHORS:\n",
        "        ax.plot(pose3d.x[pose_anchor], pose3d.y[pose_anchor], pose3d.z[pose_anchor], color=\"#ffffff\", linewidth=5)\n",
        "\n",
        "    # Add the calculated angle as text to the plot\n",
        "    fig.text(0.05, 0.95, f\"{angle_point} Angle: {joint_angle:.2f}°\", color=\"yellow\", fontsize=24,\n",
        "             verticalalignment='top', horizontalalignment='left', backgroundcolor=\"black\")\n",
        "\n",
        "    # Plot foot trace 1 if available\n",
        "    if foot_1_trace is not None:\n",
        "        foot_1_trace_x, foot_1_trace_y, foot_1_trace_z = foot_1_trace.get_state()\n",
        "        ax.plot(foot_1_trace_x, foot_1_trace_y, foot_1_trace_z, color=\"#fecea8\", linewidth=2)\n",
        "\n",
        "    # Plot foot trace 2 if available\n",
        "    if foot_2_trace is not None:\n",
        "        foot_2_trace_x, foot_2_trace_y, foot_2_trace_z = foot_2_trace.get_state()\n",
        "        ax.plot(foot_2_trace_x, foot_2_trace_y, foot_2_trace_z, color=\"#fecea8\", linewidth=2)\n",
        "\n",
        "    # Scatter plot for the pose\n",
        "    ax.scatter(pose3d.x, pose3d.y, pose3d.z, color=\"#ffffff\")\n",
        "\n",
        "    # Set the viewing angle\n",
        "    ax.view_init(30, 45 + angle*2)\n",
        "\n",
        "    # Save or show the plot\n",
        "    if save_path is None:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.savefig(save_path)\n",
        "        plt.close(fig)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q94bOqR4bfxC"
      },
      "outputs": [],
      "source": [
        "draw_3d1(pose3d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fM6SqExfb1_s"
      },
      "outputs": [],
      "source": [
        "pose_a = calibrate(extracted_data_a[63], FRAME_HEIGHT, BASELINE_HEIGHT_A, BASELINE_VERTICAL_OFFSET_A)\n",
        "pose_b = calibrate(extracted_data_b[63], FRAME_HEIGHT, BASELINE_HEIGHT_B, BASELINE_VERTICAL_OFFSET_B)\n",
        "pose_c = calibrate(extracted_data_c[63], FRAME_HEIGHT, BASELINE_HEIGHT_C, BASELINE_VERTICAL_OFFSET_C)\n",
        "\n",
        "pose3d = Pose3D.from2D(pose_a=pose_a, pose_b=pose_b, pose_c=pose_c)\n",
        "\n",
        "draw_3d1(pose3d, 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHUjxyoCcEyN"
      },
      "outputs": [],
      "source": [
        "OUTPUT_DIR = \"frames\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pax4DFEwcFlV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('..')\n",
        "%cd {HOME}\n",
        "!mkdir -p $OUTPUT_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhrnwZ0_cnws"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def save_to_json(frame_index: int, pose3d: Pose3D, json_data: dict):\n",
        "    json_data[frame_index] = {\n",
        "        \"pose\": {\n",
        "            \"x\": pose3d.x.tolist(),\n",
        "            \"y\": pose3d.y.tolist(),\n",
        "            \"z\": pose3d.z.tolist()\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "def save_to_excel(json_data: dict, excel_file_path: str, angle_data: dict):\n",
        "    # Create a list to hold the structured data\n",
        "    data = []\n",
        "\n",
        "    # Loop through the json_data to extract the frame and 3D pose information\n",
        "    for frame_index, frame_data in json_data.items():\n",
        "        # Get the x, y, z lists for the current frame\n",
        "        x_coords = frame_data['pose']['x']\n",
        "        y_coords = frame_data['pose']['y']\n",
        "        z_coords = frame_data['pose']['z']\n",
        "        angle = angle_data.get(frame_index, None)\n",
        "\n",
        "        # Combine all keypoints for this frame into rows (one row per frame)\n",
        "        for kp_idx in range(len(x_coords)):\n",
        "            data.append([frame_index, kp_idx + 1, x_coords[kp_idx], y_coords[kp_idx], z_coords[kp_idx], angle])\n",
        "\n",
        "    # Create a DataFrame with appropriate column names\n",
        "    df = pd.DataFrame(data, columns=['Frame', 'Keypoint', 'X', 'Y', 'Z', 'Elbow_Angle'])\n",
        "\n",
        "    # Save the DataFrame to an Excel file\n",
        "    df.to_excel(excel_file_path, index=False)\n",
        "    print(f\"Excel file saved to {excel_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qxk6z85v-9MI"
      },
      "outputs": [],
      "source": [
        "pip install pandas openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH3J6K9e_BP_"
      },
      "outputs": [],
      "source": [
        "from tqdm import trange\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "head_idx = 0  # Index for head keypoint\n",
        "foot_idx = 15  # Index for foot keypoint (adjust based on your dataset)\n",
        "\n",
        "def calculate_distance(point1, point2):\n",
        "    \"\"\"Calculate Euclidean distance between two 3D points.\"\"\"\n",
        "    return np.sqrt((point2[0] - point1[0]) ** 2 +\n",
        "                   (point2[1] - point1[1]) ** 2 +\n",
        "                   (point2[2] - point1[2]) ** 2)\n",
        "\n",
        "def calculate_pose_anchor_distances(pose3d, anchors):\n",
        "    \"\"\"Calculate distances between keypoint pairs defined in POSE_ANCHORS.\"\"\"\n",
        "    distances = []\n",
        "    for anchor in anchors:\n",
        "        kp1_idx, kp2_idx = anchor\n",
        "        point1 = (pose3d.x[kp1_idx], pose3d.y[kp1_idx], pose3d.z[kp1_idx])\n",
        "        point2 = (pose3d.x[kp2_idx], pose3d.y[kp2_idx], pose3d.z[kp2_idx])\n",
        "        distance = calculate_distance(point1, point2)\n",
        "        distances.append(distance)\n",
        "    return distances\n",
        "\n",
        "def calculate_person_height_in_pixels(pose3d):\n",
        "    \"\"\"Calculate the height of a person in pixels using 3D pose keypoints.\"\"\"\n",
        "    head_point = (pose3d.x[head_idx], pose3d.y[head_idx], pose3d.z[head_idx])\n",
        "    foot_point = (pose3d.x[foot_idx], pose3d.y[foot_idx], pose3d.z[foot_idx])\n",
        "\n",
        "    # Calculate the distance between the head and foot in 3D space (in pixels)\n",
        "    pixel_height = calculate_distance(head_point, foot_point)\n",
        "    return pixel_height\n",
        "\n",
        "# Step 2: Convert pixel height to real height (meters)\n",
        "def calculate_real_height(pixel_height, pixel_to_meter_ratio):\n",
        "    \"\"\"Convert pixel height to real-world height (in meters).\"\"\"\n",
        "    real_height = pixel_height * pixel_to_meter_ratio\n",
        "    return real_height\n",
        "\n",
        "\n",
        "def calculate_angle(a, b, c):\n",
        "    \"\"\"Calculate angle ABC given three points A, B, C.\"\"\"\n",
        "    ab = np.array(b) - np.array(a)\n",
        "    cb = np.array(b) - np.array(c)\n",
        "\n",
        "    # Use the dot product to find the cosine of the angle\n",
        "    cosine_angle = np.dot(ab, cb) / (np.linalg.norm(ab) * np.linalg.norm(cb))\n",
        "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))  # Clip for numerical stability\n",
        "    return np.degrees(angle)  # Convert from radians to degrees\n",
        "\n",
        "\n",
        "# Example usage\n",
        "pose3d = Pose3D.from2D(pose_a=pose_a, pose_b=pose_b, pose_c=pose_c)  # Example pose data\n",
        "pixel_height = calculate_person_height_in_pixels(pose3d)\n",
        "\n",
        "# Assuming a pixel-to-meter ratio, e.g., 0.002 meters per pixel\n",
        "# pixel_to_meter_ratio = object_height_in_meters / object_height_in_pixels\n",
        "pixel_to_meter_ratio = 0.002  # meters per pixel (this should be calculated or calibrated)\n",
        "\n",
        "person_real_height = calculate_real_height(pixel_height, pixel_to_meter_ratio)\n",
        "print(f\"The person's estimated height is: {person_real_height} meters\")\n",
        "\n",
        "def generate_frames(start=0, end=100):\n",
        "    foot_1_trace = Trace(10)\n",
        "    foot_2_trace = Trace(10)\n",
        "    json_data = {}\n",
        "    angle_data = {}\n",
        "\n",
        "    # Initialize an empty DataFrame for storing distances\n",
        "    distance_columns = ['Frame'] + [f'Distance_{i}' for i in range(len(POSE_ANCHORS))]\n",
        "    distance_df = pd.DataFrame(columns=distance_columns)\n",
        "\n",
        "    for i in trange(start, end):\n",
        "        a = calibrate(extracted_data_a[i], FRAME_HEIGHT, BASELINE_HEIGHT_A, BASELINE_VERTICAL_OFFSET_A)\n",
        "        b = calibrate(extracted_data_b[i], FRAME_HEIGHT, BASELINE_HEIGHT_B, BASELINE_VERTICAL_OFFSET_B)\n",
        "        c = calibrate(extracted_data_c[i], FRAME_HEIGHT, BASELINE_HEIGHT_C, BASELINE_VERTICAL_OFFSET_C)\n",
        "\n",
        "        if a is None or b is None:\n",
        "            continue\n",
        "\n",
        "        pose_a = a\n",
        "        pose_b = b\n",
        "        pose_c = c\n",
        "\n",
        "        pose3d = Pose3D.from2D(pose_a=pose_a, pose_b=pose_b, pose_c=pose_c)\n",
        "        foot_1_trace.append(x=pose3d.x[-1], y=pose3d.y[-1], z=pose3d.z[-1])\n",
        "        foot_2_trace.append(x=pose3d.x[-2], y=pose3d.y[-2], z=pose3d.z[-2])\n",
        "\n",
        "        # Calculate distances for the pose keypoints\n",
        "        distances = calculate_pose_anchor_distances(pose3d, POSE_ANCHORS)\n",
        "        new_row = pd.DataFrame([[i] + distances], columns=distance_df.columns)\n",
        "        distance_df = pd.concat([distance_df, new_row], ignore_index=True)\n",
        "\n",
        "        save_to_json(i, pose3d, json_data)\n",
        "\n",
        "        shoulder_idx = 8\n",
        "        elbow_idx = 6\n",
        "        wrist_idx = 12\n",
        "\n",
        "        # Calculate the elbow angles in 2D for each pose\n",
        "        elbow_angle_a = calculate_angle(\n",
        "            (pose_a.x[shoulder_idx], pose_a.y[shoulder_idx]),\n",
        "            (pose_a.x[elbow_idx], pose_a.y[elbow_idx]),\n",
        "            (pose_a.x[wrist_idx], pose_a.y[wrist_idx])\n",
        "        )\n",
        "\n",
        "        elbow_angle_b = calculate_angle(\n",
        "            (pose_b.x[shoulder_idx], pose_b.y[shoulder_idx]),\n",
        "            (pose_b.x[elbow_idx], pose_b.y[elbow_idx]),\n",
        "            (pose_b.x[wrist_idx], pose_b.y[wrist_idx])\n",
        "        )\n",
        "\n",
        "        elbow_angle_c = calculate_angle(\n",
        "            (pose_c.x[shoulder_idx], pose_c.y[shoulder_idx]),\n",
        "            (pose_c.x[elbow_idx], pose_c.y[elbow_idx]),\n",
        "            (pose_c.x[wrist_idx], pose_c.y[wrist_idx])\n",
        "        )\n",
        "\n",
        "        # Calculate the elbow angle in 3D for the reconstructed pose\n",
        "        # elbow_angle_3d = calculate_angle(\n",
        "        #     (pose3d.x[shoulder_idx], pose3d.y[shoulder_idx], pose3d.z[shoulder_idx]),\n",
        "        #     (pose3d.x[elbow_idx], pose3d.y[elbow_idx], pose3d.z[elbow_idx]),\n",
        "        #     (pose3d.x[wrist_idx], pose3d.y[wrist_idx], pose3d.z[wrist_idx])\n",
        "        # )\n",
        "        elbow_angle_3d = calculate_angle_3d(\n",
        "            (pose3d.x[shoulder_idx], pose3d.y[shoulder_idx], pose3d.z[shoulder_idx]),\n",
        "            (pose3d.x[elbow_idx], pose3d.y[elbow_idx], pose3d.z[elbow_idx]),\n",
        "            (pose3d.x[wrist_idx], pose3d.y[wrist_idx], pose3d.z[wrist_idx])\n",
        "        )\n",
        "\n",
        "        # Store all the calculated elbow angles in the angle_data dictionary\n",
        "        angle_data[i] = {\n",
        "            'shoulder_angle_a': elbow_angle_a,\n",
        "            'shoulder_angle_b': elbow_angle_b,\n",
        "            'shoulder_angle_c': elbow_angle_c,\n",
        "            'shoulder_angle_3d': elbow_angle_3d,\n",
        "        }\n",
        "\n",
        "        file_name = f\"file{str(i).zfill(4)}.png\"\n",
        "        # draw_3d1(\n",
        "        #     pose3d,\n",
        "        #     angle=i // 2,\n",
        "        #     save_path=f\"{HOME}/{OUTPUT_DIR}/{file_name}\",\n",
        "        #     foot_1_trace=foot_1_trace,\n",
        "        #     foot_2_trace=foot_2_trace\n",
        "        # )\n",
        "        draw_3d1(\n",
        "            pose3d,\n",
        "            angle=i // 2,\n",
        "            joint_angle=elbow_angle_3d,\n",
        "            save_path=f\"{HOME}/{OUTPUT_DIR}/{file_name}\",\n",
        "            angle_point=f\"Right Shoulder\"\n",
        "        )\n",
        "\n",
        "    with open(f\"{HOME}/{OUTPUT_DIR}/coordinates.json\", 'w') as json_file:\n",
        "        json.dump(json_data, json_file, indent=4)\n",
        "\n",
        "    with open(f\"{HOME}/{OUTPUT_DIR}/angles.json\", 'w') as angle_file:\n",
        "        json.dump(angle_data, angle_file, indent=4)\n",
        "\n",
        "    excel_file_path = f\"{HOME}/{OUTPUT_DIR}/coordinates.xlsx\"\n",
        "    save_to_excel(json_data, excel_file_path, angle_data)\n",
        "\n",
        "    excel_file_path = f\"{HOME}/pose_distances.xlsx\"\n",
        "    distance_df.to_excel(excel_file_path, index=False)\n",
        "    print(f\"Excel file saved to {excel_file_path}\")\n",
        "\n",
        "    angle_df = pd.DataFrame.from_dict(angle_data, orient='index')\n",
        "    angle_excel_file_path = f\"{HOME}/{OUTPUT_DIR}/angles.xlsx\"\n",
        "    angle_df.to_excel(angle_excel_file_path, index=True)\n",
        "    print(f\"Angle data saved to {angle_excel_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wgUnufBcoxo"
      },
      "outputs": [],
      "source": [
        "# generate_frames(0, len(extracted_data_a))\n",
        "# generate_frames(0, len(extracted_data_b))\n",
        "generate_frames(0, len(extracted_data_c))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWJAh2t9crNI"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "def create_video_from_images(image_folder, video_name, fps=30):\n",
        "    images = [img for img in sorted(os.listdir(image_folder)) if img.endswith(\".png\")]\n",
        "\n",
        "    if not images:\n",
        "        print(\"No PNG images found in the specified directory.\")\n",
        "        return\n",
        "\n",
        "    frame = cv2.imread(os.path.join(image_folder, images[0]))\n",
        "\n",
        "    if frame is None:\n",
        "        print(\"Error reading the first image.\")\n",
        "        return\n",
        "\n",
        "    height, width, _ = frame.shape\n",
        "    video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "\n",
        "    for image in images:\n",
        "        img_path = os.path.join(image_folder, image)\n",
        "        img_frame = cv2.imread(img_path)\n",
        "        if img_frame is None:\n",
        "            print(f\"Error reading image: {img_path}\")\n",
        "            continue\n",
        "        video.write(img_frame)\n",
        "\n",
        "    video.release()\n",
        "    print(f\"Video saved as: {video_name}\")\n",
        "\n",
        "# Call the function:\n",
        "create_video_from_images(OUTPUT_DIR, \"output_video.mp4\", fps=30)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
